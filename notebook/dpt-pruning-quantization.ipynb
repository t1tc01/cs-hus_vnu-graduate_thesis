{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2992108,"sourceType":"datasetVersion","datasetId":1833436},{"sourceId":2993779,"sourceType":"datasetVersion","datasetId":1833438},{"sourceId":2993787,"sourceType":"datasetVersion","datasetId":1833442},{"sourceId":8105715,"sourceType":"datasetVersion","datasetId":4787395},{"sourceId":8157630,"sourceType":"datasetVersion","datasetId":4825820}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"print('Installing torchprofile...')\n!pip install torchprofile 1>/dev/null\nprint('All required packages have been successfully installed!')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-15T14:32:16.445829Z","iopub.execute_input":"2024-05-15T14:32:16.446460Z","iopub.status.idle":"2024-05-15T14:32:30.035027Z","shell.execute_reply.started":"2024-05-15T14:32:16.446428Z","shell.execute_reply":"2024-05-15T14:32:30.033893Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Installing torchprofile...\nAll required packages have been successfully installed!\n","output_type":"stream"}]},{"cell_type":"code","source":"import copy\nimport math\nimport random\nimport time\nfrom collections import OrderedDict, defaultdict\nfrom typing import Union, List\n\nimport numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\nfrom torch import nn\nfrom torch.optim import *\nfrom torch.optim.lr_scheduler import *\nfrom torch.utils.data import DataLoader\nfrom torchprofile import profile_macs\nfrom torchvision.datasets import *\nfrom torchvision.transforms import *\nfrom tqdm.auto import tqdm\n\nfrom torchprofile import profile_macs\n\nassert torch.cuda.is_available(), \\\n\"The current runtime does not have CUDA support.\" \\\n\"Please go to menu bar (Runtime - Change runtime type) and select GPU\"","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:30.037171Z","iopub.execute_input":"2024-05-15T14:32:30.037472Z","iopub.status.idle":"2024-05-15T14:32:35.911529Z","shell.execute_reply.started":"2024-05-15T14:32:30.037443Z","shell.execute_reply":"2024-05-15T14:32:35.910462Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"random.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:35.912895Z","iopub.execute_input":"2024-05-15T14:32:35.913731Z","iopub.status.idle":"2024-05-15T14:32:35.923744Z","shell.execute_reply.started":"2024-05-15T14:32:35.913694Z","shell.execute_reply":"2024-05-15T14:32:35.922902Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7c222dfef3f0>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Load Model","metadata":{}},{"cell_type":"code","source":"from transformers import DPTImageProcessor, DPTForDepthEstimation\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport requests","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:35.926604Z","iopub.execute_input":"2024-05-15T14:32:35.927215Z","iopub.status.idle":"2024-05-15T14:32:47.017263Z","shell.execute_reply.started":"2024-05-15T14:32:35.927190Z","shell.execute_reply":"2024-05-15T14:32:47.016232Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-05-15 14:32:38.036933: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-15 14:32:38.037049: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-15 14:32:38.163260: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-swinv2-tiny-256\")\nmodel = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-swinv2-tiny-256\").to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:47.018629Z","iopub.execute_input":"2024-05-15T14:32:47.019315Z","iopub.status.idle":"2024-05-15T14:32:49.558728Z","shell.execute_reply.started":"2024-05-15T14:32:47.019281Z","shell.execute_reply":"2024-05-15T14:32:49.557904Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/425 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c2fb00df681437f94edb44838d1393a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd8ea19216de4c7eb35b428a735304f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/164M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11f1ff77bbd646fcaf995dbc97f77087"}},"metadata":{}}]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:49.560096Z","iopub.execute_input":"2024-05-15T14:32:49.560563Z","iopub.status.idle":"2024-05-15T14:32:49.575102Z","shell.execute_reply.started":"2024-05-15T14:32:49.560526Z","shell.execute_reply":"2024-05-15T14:32:49.574261Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DPTForDepthEstimation(\n  (backbone): Swinv2Backbone(\n    (embeddings): Swinv2Embeddings(\n      (patch_embeddings): Swinv2PatchEmbeddings(\n        (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n      )\n      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): Swinv2Encoder(\n      (layers): ModuleList(\n        (0): Swinv2Stage(\n          (blocks): ModuleList(\n            (0-1): 2 x Swinv2Layer(\n              (attention): Swinv2Attention(\n                (self): Swinv2SelfAttention(\n                  (continuous_position_bias_mlp): Sequential(\n                    (0): Linear(in_features=2, out_features=512, bias=True)\n                    (1): ReLU(inplace=True)\n                    (2): Linear(in_features=512, out_features=3, bias=False)\n                  )\n                  (query): Linear(in_features=96, out_features=96, bias=True)\n                  (key): Linear(in_features=96, out_features=96, bias=False)\n                  (value): Linear(in_features=96, out_features=96, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): Swinv2SelfOutput(\n                  (dense): Linear(in_features=96, out_features=96, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n              (drop_path): Swinv2DropPath(p=0.1)\n              (intermediate): Swinv2Intermediate(\n                (dense): Linear(in_features=96, out_features=384, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): Swinv2Output(\n                (dense): Linear(in_features=384, out_features=96, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n          (downsample): Swinv2PatchMerging(\n            (reduction): Linear(in_features=384, out_features=192, bias=False)\n            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (1): Swinv2Stage(\n          (blocks): ModuleList(\n            (0-1): 2 x Swinv2Layer(\n              (attention): Swinv2Attention(\n                (self): Swinv2SelfAttention(\n                  (continuous_position_bias_mlp): Sequential(\n                    (0): Linear(in_features=2, out_features=512, bias=True)\n                    (1): ReLU(inplace=True)\n                    (2): Linear(in_features=512, out_features=6, bias=False)\n                  )\n                  (query): Linear(in_features=192, out_features=192, bias=True)\n                  (key): Linear(in_features=192, out_features=192, bias=False)\n                  (value): Linear(in_features=192, out_features=192, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): Swinv2SelfOutput(\n                  (dense): Linear(in_features=192, out_features=192, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n              (drop_path): Swinv2DropPath(p=0.1)\n              (intermediate): Swinv2Intermediate(\n                (dense): Linear(in_features=192, out_features=768, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): Swinv2Output(\n                (dense): Linear(in_features=768, out_features=192, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n          (downsample): Swinv2PatchMerging(\n            (reduction): Linear(in_features=768, out_features=384, bias=False)\n            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (2): Swinv2Stage(\n          (blocks): ModuleList(\n            (0-5): 6 x Swinv2Layer(\n              (attention): Swinv2Attention(\n                (self): Swinv2SelfAttention(\n                  (continuous_position_bias_mlp): Sequential(\n                    (0): Linear(in_features=2, out_features=512, bias=True)\n                    (1): ReLU(inplace=True)\n                    (2): Linear(in_features=512, out_features=12, bias=False)\n                  )\n                  (query): Linear(in_features=384, out_features=384, bias=True)\n                  (key): Linear(in_features=384, out_features=384, bias=False)\n                  (value): Linear(in_features=384, out_features=384, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): Swinv2SelfOutput(\n                  (dense): Linear(in_features=384, out_features=384, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (drop_path): Swinv2DropPath(p=0.1)\n              (intermediate): Swinv2Intermediate(\n                (dense): Linear(in_features=384, out_features=1536, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): Swinv2Output(\n                (dense): Linear(in_features=1536, out_features=384, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n          (downsample): Swinv2PatchMerging(\n            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (3): Swinv2Stage(\n          (blocks): ModuleList(\n            (0-1): 2 x Swinv2Layer(\n              (attention): Swinv2Attention(\n                (self): Swinv2SelfAttention(\n                  (continuous_position_bias_mlp): Sequential(\n                    (0): Linear(in_features=2, out_features=512, bias=True)\n                    (1): ReLU(inplace=True)\n                    (2): Linear(in_features=512, out_features=24, bias=False)\n                  )\n                  (query): Linear(in_features=768, out_features=768, bias=True)\n                  (key): Linear(in_features=768, out_features=768, bias=False)\n                  (value): Linear(in_features=768, out_features=768, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): Swinv2SelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (drop_path): Swinv2DropPath(p=0.1)\n              (intermediate): Swinv2Intermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): Swinv2Output(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n      )\n    )\n  )\n  (neck): DPTNeck(\n    (convs): ModuleList(\n      (0): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (2): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (3): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    )\n    (fusion_stage): DPTFeatureFusionStage(\n      (layers): ModuleList(\n        (0-3): 4 x DPTFeatureFusionLayer(\n          (projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n          (residual_layer1): DPTPreActResidualLayer(\n            (activation1): ReLU()\n            (convolution1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (activation2): ReLU()\n            (convolution2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n          (residual_layer2): DPTPreActResidualLayer(\n            (activation1): ReLU()\n            (convolution1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (activation2): ReLU()\n            (convolution2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n      )\n    )\n  )\n  (head): DPTDepthEstimationHead(\n    (head): Sequential(\n      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): Upsample(scale_factor=2.0, mode='bilinear')\n      (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): ReLU()\n      (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n      (5): ReLU()\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"config ={\n    \"General\":{\n        \"device\":\"cuda\",\n        \"type\":\"full\",\n        \"model\": model,\n        \"emb_dim\":768,\n        \"hooks\":[2, 5, 8, 11],\n        \"read\":\"projection\",\n        \"resample_dim\":256,\n        \"optim\":\"adam\",\n        \"lr_backbone\":1e-5,\n        \"lr_scratch\":3e-4,\n        \"loss_depth\":\"ssi\",\n        \"loss_segmentation\":\"ce\",\n        \"momentum\":0.9,\n        \"epochs\":3,\n        \"batch_size\":1,\n        \"path_model\":\"models\",\n        \"path_predicted_images\":\"output\",\n        \"seed\":0,\n        \"patch_size\":16\n    },\n    \"Dataset\":{\n        \"paths\":{\n            \"path_dataset\":\"/kaggle/input\",\n            \"list_datasets\":[\"inria-fod\", \"nyuv2-fod\", \"posetrack-fod\"],\n            \"path_images\":\"images\",\n            \"path_segmentations\":\"segmentations\",\n            \"path_depths\":\"depths\"\n        },\n        \"extensions\":{\n            \"ext_images\":\".jpg\",\n            \"ext_segmentations\":\".png\",\n            \"ext_depths\":\".jpg\"\n        },\n        \"splits\":{\n            \"split_train\":0.6,\n            \"split_val\":0.2,\n            \"split_test\":0.2\n        },\n        \"transforms\":{\n            \"resize\":256,\n            \"p_flip\":0.5,\n            \"p_crop\":0.3,\n            \"p_rot\":0.2\n        },\n        \"classes\":{\n            \"1\": {\n                \"name\": \"person\",\n                \"color\": [150,5,61]\n            }\n        }\n    },\n    \"wandb\":{\n        \"enable\": False,\n        \"username\":\"younesbelkada\",\n        \"images_to_show\":3,\n        \"im_h\":540,\n        \"im_w\":980\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:49.576576Z","iopub.execute_input":"2024-05-15T14:32:49.577124Z","iopub.status.idle":"2024-05-15T14:32:49.632027Z","shell.execute_reply.started":"2024-05-15T14:32:49.577098Z","shell.execute_reply":"2024-05-15T14:32:49.631126Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Helper function","metadata":{}},{"cell_type":"code","source":"import os, errno\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom glob import glob\nfrom PIL import Image\nfrom torchvision import transforms, utils","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:49.633177Z","iopub.execute_input":"2024-05-15T14:32:49.633515Z","iopub.status.idle":"2024-05-15T14:32:49.644005Z","shell.execute_reply.started":"2024-05-15T14:32:49.633485Z","shell.execute_reply":"2024-05-15T14:32:49.643143Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def get_total_paths(path, ext):\n    return glob(os.path.join(path, '*'+ext))","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:49.645043Z","iopub.execute_input":"2024-05-15T14:32:49.645282Z","iopub.status.idle":"2024-05-15T14:32:49.653307Z","shell.execute_reply.started":"2024-05-15T14:32:49.645261Z","shell.execute_reply":"2024-05-15T14:32:49.652511Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def get_transforms(config):\n    im_size = config['Dataset']['transforms']['resize']\n    transform_image = transforms.Compose([\n        transforms.Resize((im_size, im_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ])\n    transform_depth = transforms.Compose([\n        transforms.Resize((im_size, im_size)),\n        transforms.Grayscale(num_output_channels=1) ,\n        transforms.ToTensor()\n    ])\n    transform_seg = transforms.Compose([\n        transforms.Resize((im_size, im_size), interpolation=transforms.InterpolationMode.NEAREST),\n        ToMask(config['Dataset']['classes']),\n    ])\n    return transform_image, transform_depth, transform_seg","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:49.656579Z","iopub.execute_input":"2024-05-15T14:32:49.657178Z","iopub.status.idle":"2024-05-15T14:32:49.664106Z","shell.execute_reply.started":"2024-05-15T14:32:49.657152Z","shell.execute_reply":"2024-05-15T14:32:49.663372Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def get_splitted_dataset(config, split, dataset_name, path_images, path_depths, path_segmentation):\n    list_files = [os.path.basename(im) for im in path_images]\n    np.random.seed(config['General']['seed'])\n    np.random.shuffle(list_files)\n    if split == 'train':\n        selected_files = list_files[:int(len(list_files)*config['Dataset']['splits']['split_train'])]\n    elif split == 'val':\n        selected_files = list_files[int(len(list_files)*config['Dataset']['splits']['split_train']):int(len(list_files)*config['Dataset']['splits']['split_train'])+int(len(list_files)*config['Dataset']['splits']['split_val'])]\n    else:\n        selected_files = list_files[int(len(list_files)*config['Dataset']['splits']['split_train'])+int(len(list_files)*config['Dataset']['splits']['split_val']):]\n\n    path_images = [os.path.join(config['Dataset']['paths']['path_dataset'], dataset_name, config['Dataset']['paths']['path_images'], im[:-4]+config['Dataset']['extensions']['ext_images']) for im in selected_files]\n    path_depths = [os.path.join(config['Dataset']['paths']['path_dataset'], dataset_name, config['Dataset']['paths']['path_depths'], im[:-4]+config['Dataset']['extensions']['ext_depths']) for im in selected_files]\n    path_segmentation = [os.path.join(config['Dataset']['paths']['path_dataset'], dataset_name, config['Dataset']['paths']['path_segmentations'], im[:-4]+config['Dataset']['extensions']['ext_segmentations']) for im in selected_files]\n    return path_images, path_depths, path_segmentation","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:49.665132Z","iopub.execute_input":"2024-05-15T14:32:49.665442Z","iopub.status.idle":"2024-05-15T14:32:49.679275Z","shell.execute_reply.started":"2024-05-15T14:32:49.665418Z","shell.execute_reply":"2024-05-15T14:32:49.678437Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Custom augmentation","metadata":{}},{"cell_type":"code","source":"class ToMask(object):\n    \"\"\"\n        Convert a 3 channel RGB image into a 1 channel segmentation mask\n    \"\"\"\n    def __init__(self, palette_dictionnary):\n        self.nb_classes = len(palette_dictionnary)\n        # sort the dictionary of the classes by the sum of rgb value -> to have always background = 0\n        # self.converted_dictionnary = {i: v for i, (k, v) in enumerate(sorted(palette_dictionnary.items(), key=lambda item: sum(item[1])))}\n        self.palette_dictionnary = palette_dictionnary\n\n    def __call__(self, pil_image):\n        # avoid taking the alpha channel\n        image_array = np.array(pil_image)[:, :, :3]\n        # get only one channel for the output\n        output_array = np.zeros(image_array.shape, dtype=\"int\")[:, :, 0]\n\n        for label in self.palette_dictionnary.keys():\n            rgb_color = self.palette_dictionnary[label]['color']\n            mask = (image_array == rgb_color)\n            output_array[mask[:, :, 0]] = int(label)\n\n        output_array = torch.from_numpy(output_array).unsqueeze(0).long()\n        return output_array","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:49.680285Z","iopub.execute_input":"2024-05-15T14:32:49.680600Z","iopub.status.idle":"2024-05-15T14:32:49.692202Z","shell.execute_reply.started":"2024-05-15T14:32:49.680576Z","shell.execute_reply":"2024-05-15T14:32:49.691314Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nfrom glob import glob\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom torch.utils.data.dataloader import default_collate\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torchvision.transforms.functional as TF","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:49.693277Z","iopub.execute_input":"2024-05-15T14:32:49.693537Z","iopub.status.idle":"2024-05-15T14:32:49.705321Z","shell.execute_reply.started":"2024-05-15T14:32:49.693516Z","shell.execute_reply":"2024-05-15T14:32:49.704487Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.utils.data import ConcatDataset","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:49.706299Z","iopub.execute_input":"2024-05-15T14:32:49.706640Z","iopub.status.idle":"2024-05-15T14:32:49.715041Z","shell.execute_reply.started":"2024-05-15T14:32:49.706617Z","shell.execute_reply":"2024-05-15T14:32:49.714195Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def show(imgs):\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = transforms.ToPILImage()(img.to('cpu').float())\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:49.716154Z","iopub.execute_input":"2024-05-15T14:32:49.716438Z","iopub.status.idle":"2024-05-15T14:32:49.724557Z","shell.execute_reply.started":"2024-05-15T14:32:49.716416Z","shell.execute_reply":"2024-05-15T14:32:49.723813Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class AutoFocusDataset(Dataset):\n    \"\"\"\n        Dataset class for the AutoFocus Task. Requires for each image, its depth ground-truth and\n        segmentation mask\n        Args:\n            :- config -: json config file\n            :- dataset_name -: str\n            :- split -: split ['train', 'val', 'test']\n    \"\"\"\n    def __init__(self, config, dataset_name, split=None):\n        self.split = split\n        self.config = config\n\n        path_images = os.path.join(config['Dataset']['paths']['path_dataset'], dataset_name, config['Dataset']['paths']['path_images'])\n        path_depths = os.path.join(config['Dataset']['paths']['path_dataset'], dataset_name, config['Dataset']['paths']['path_depths'])\n        path_segmentations = os.path.join(config['Dataset']['paths']['path_dataset'], dataset_name, config['Dataset']['paths']['path_segmentations'])\n        \n        self.paths_images = get_total_paths(path_images, config['Dataset']['extensions']['ext_images'])\n        self.paths_depths = get_total_paths(path_depths, config['Dataset']['extensions']['ext_depths'])\n        self.paths_segmentations = get_total_paths(path_segmentations, config['Dataset']['extensions']['ext_segmentations'])\n        \n        assert (self.split in ['train', 'test', 'val']), \"Invalid split!\"\n        assert (len(self.paths_images) == len(self.paths_depths)), \"Different number of instances between the input and the depth maps\"\n        assert (len(self.paths_images) == len(self.paths_segmentations)), \"Different number of instances between the input and the segmentation maps\"\n        assert (config['Dataset']['splits']['split_train']+config['Dataset']['splits']['split_test']+config['Dataset']['splits']['split_val'] == 1), \"Invalid splits (sum must be equal to 1)\"\n        # check for segmentation\n\n      \n        # utility func for splitting\n        self.paths_images, self.paths_depths, self.paths_segmentations = get_splitted_dataset(config, self.split, dataset_name, self.paths_images, self.paths_depths, self.paths_segmentations)\n\n        #----------------------------------------------------------------------------------------------------------------------------\n        # Get the transforms\n        self.transform_image, self.transform_depth, self.transform_seg = get_transforms(config)\n\n        # get p_flip from config\n        self.p_flip = config['Dataset']['transforms']['p_flip'] if split=='train' else 0\n        self.p_crop = config['Dataset']['transforms']['p_crop'] if split=='train' else 0\n        self.p_rot = config['Dataset']['transforms']['p_rot'] if split=='train' else 0\n        self.resize = config['Dataset']['transforms']['resize']\n        \n    def __len__(self):\n        \"\"\"\n            Function to get the number of images using the given list of images\n        \"\"\"\n        return len(self.paths_images)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n            Getter function in order to get the triplet of images / depth maps and segmentation masks\n        \"\"\"\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        image = self.transform_image(Image.open(self.paths_images[idx]))\n        depth = self.transform_depth(Image.open(self.paths_depths[idx]))\n        segmentation = self.transform_seg(Image.open(self.paths_segmentations[idx]))\n        imgorig = image.clone()\n\n        if random.random() < self.p_flip:\n            image = TF.hflip(image)\n            depth = TF.hflip(depth)\n            segmentation = TF.hflip(segmentation)\n\n        if random.random() < self.p_crop:\n            random_size = random.randint(128, self.resize-1)\n            max_size = self.resize - random_size\n            left = int(random.random()*max_size)\n            top = int(random.random()*max_size)\n            image = TF.crop(image, top, left, random_size, random_size)\n            depth = TF.crop(depth, top, left, random_size, random_size)\n            segmentation = TF.crop(segmentation, top, left, random_size, random_size)\n            image = transforms.Resize((self.resize, self.resize))(image)\n            depth = transforms.Resize((self.resize, self.resize))(depth)\n            segmentation = transforms.Resize((self.resize, self.resize), interpolation=transforms.InterpolationMode.NEAREST)(segmentation)\n\n        if random.random() < self.p_rot:\n            #rotate\n            random_angle = random.random()*20 - 10 #[-10 ; 10]\n            mask = torch.ones((1,self.resize,self.resize)) #useful for the resize at the end\n            mask = TF.rotate(mask, random_angle, interpolation=transforms.InterpolationMode.BILINEAR)\n            image = TF.rotate(image, random_angle, interpolation=transforms.InterpolationMode.BILINEAR)\n            depth = TF.rotate(depth, random_angle, interpolation=transforms.InterpolationMode.BILINEAR)\n            segmentation = TF.rotate(segmentation, random_angle, interpolation=transforms.InterpolationMode.NEAREST)\n            #crop to remove black borders due to the rotation\n            left = torch.argmax(mask[:,0,:]).item()\n            top = torch.argmax(mask[:,:,0]).item()\n            coin = min(left,top)\n            size = self.resize - 2*coin\n            image = TF.crop(image, coin, coin, size, size)\n            depth = TF.crop(depth, coin, coin, size, size)\n            segmentation = TF.crop(segmentation, coin, coin, size, size)\n            #Resize\n            image = transforms.Resize((self.resize, self.resize))(image)\n            depth = transforms.Resize((self.resize, self.resize))(depth)\n            segmentation = transforms.Resize((self.resize, self.resize), interpolation=transforms.InterpolationMode.NEAREST)(segmentation)\n        # show([imgorig, image, depth, segmentation])\n        # exit(0)\n        return image, depth, segmentation","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:49.725693Z","iopub.execute_input":"2024-05-15T14:32:49.725971Z","iopub.status.idle":"2024-05-15T14:32:49.750953Z","shell.execute_reply.started":"2024-05-15T14:32:49.725949Z","shell.execute_reply":"2024-05-15T14:32:49.750249Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"list_data = config['Dataset']['paths']['list_datasets']\nlist_data","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:49.752240Z","iopub.execute_input":"2024-05-15T14:32:49.752494Z","iopub.status.idle":"2024-05-15T14:32:49.766267Z","shell.execute_reply.started":"2024-05-15T14:32:49.752472Z","shell.execute_reply":"2024-05-15T14:32:49.765402Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"['inria-fod', 'nyuv2-fod', 'posetrack-fod']"},"metadata":{}}]},{"cell_type":"code","source":"autofocus_datasets_train = [] #include 3 dataset \nfor dataset_name in list_data:\n     autofocus_datasets_train.append(AutoFocusDataset(config, dataset_name, 'train'))\ntrain_data = ConcatDataset(autofocus_datasets_train)\ntrain_dataloader = DataLoader(train_data, batch_size=config['General']['batch_size'], shuffle=True)\nlen(train_data)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:49.767570Z","iopub.execute_input":"2024-05-15T14:32:49.767844Z","iopub.status.idle":"2024-05-15T14:32:54.950855Z","shell.execute_reply.started":"2024-05-15T14:32:49.767821Z","shell.execute_reply":"2024-05-15T14:32:54.949946Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"1512"},"metadata":{}}]},{"cell_type":"code","source":"autofocus_datasets_test = [] #include 3 dataset \nfor dataset_name in list_data:\n     autofocus_datasets_test.append(AutoFocusDataset(config, dataset_name, 'test'))\ntest_data = ConcatDataset(autofocus_datasets_train)\ntest_dataloader = DataLoader(test_data, batch_size=config['General']['batch_size'], shuffle=True)\nlen(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:54.952133Z","iopub.execute_input":"2024-05-15T14:32:54.952504Z","iopub.status.idle":"2024-05-15T14:32:54.997046Z","shell.execute_reply.started":"2024-05-15T14:32:54.952469Z","shell.execute_reply":"2024-05-15T14:32:54.996195Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"1512"},"metadata":{}}]},{"cell_type":"code","source":"autofocus_datasets_val = []\nfor dataset_name in list_data:\n    autofocus_datasets_val.append(AutoFocusDataset(config, dataset_name, 'val'))\nval_data = ConcatDataset(autofocus_datasets_val)\nval_dataloader = DataLoader(val_data, batch_size=config['General']['batch_size'], shuffle=True)\nlen(val_data)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:54.998251Z","iopub.execute_input":"2024-05-15T14:32:54.999025Z","iopub.status.idle":"2024-05-15T14:32:55.043414Z","shell.execute_reply.started":"2024-05-15T14:32:54.998993Z","shell.execute_reply":"2024-05-15T14:32:55.042576Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"504"},"metadata":{}}]},{"cell_type":"markdown","source":"Loss function","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:55.044379Z","iopub.execute_input":"2024-05-15T14:32:55.044655Z","iopub.status.idle":"2024-05-15T14:32:55.048673Z","shell.execute_reply.started":"2024-05-15T14:32:55.044633Z","shell.execute_reply":"2024-05-15T14:32:55.047576Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def compute_scale_and_shift(prediction, target, mask):\n    # system matrix: A = [[a_00, a_01], [a_10, a_11]]\n    a_00 = torch.sum(mask * prediction * prediction, (1, 2))\n    a_01 = torch.sum(mask * prediction, (1, 2))\n    a_11 = torch.sum(mask, (1, 2))\n\n    # right hand side: b = [b_0, b_1]\n    b_0 = torch.sum(mask * prediction * target, (1, 2))\n    b_1 = torch.sum(mask * target, (1, 2))\n\n    # solution: x = A^-1 . b = [[a_11, -a_01], [-a_10, a_00]] / (a_00 * a_11 - a_01 * a_10) . b\n    x_0 = torch.zeros_like(b_0)\n    x_1 = torch.zeros_like(b_1)\n\n    det = a_00 * a_11 - a_01 * a_01\n    valid = det.nonzero()\n\n    x_0[valid] = (a_11[valid] * b_0[valid] - a_01[valid] * b_1[valid]) / det[valid]\n    x_1[valid] = (-a_01[valid] * b_0[valid] + a_00[valid] * b_1[valid]) / det[valid]\n\n    return x_0, x_1","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:55.049590Z","iopub.execute_input":"2024-05-15T14:32:55.049830Z","iopub.status.idle":"2024-05-15T14:32:55.059451Z","shell.execute_reply.started":"2024-05-15T14:32:55.049808Z","shell.execute_reply":"2024-05-15T14:32:55.058597Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def reduction_batch_based(image_loss, M):\n    # average of all valid pixels of the batch\n\n    # avoid division by 0 (if sum(M) = sum(sum(mask)) = 0: sum(image_loss) = 0)\n    divisor = torch.sum(M)\n\n    if divisor == 0:\n        return 0\n    else:\n        return torch.sum(image_loss) / divisor","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:55.060409Z","iopub.execute_input":"2024-05-15T14:32:55.060673Z","iopub.status.idle":"2024-05-15T14:32:55.069823Z","shell.execute_reply.started":"2024-05-15T14:32:55.060650Z","shell.execute_reply":"2024-05-15T14:32:55.069099Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def reduction_image_based(image_loss, M):\n    # mean of average of valid pixels of an image\n\n    # avoid division by 0 (if M = sum(mask) = 0: image_loss = 0)\n    valid = M.nonzero()\n\n    image_loss[valid] = image_loss[valid] / M[valid]\n\n    return torch.mean(image_loss)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:55.070752Z","iopub.execute_input":"2024-05-15T14:32:55.071033Z","iopub.status.idle":"2024-05-15T14:32:55.083610Z","shell.execute_reply.started":"2024-05-15T14:32:55.071011Z","shell.execute_reply":"2024-05-15T14:32:55.082809Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def mse_loss(prediction, target, mask, reduction=reduction_batch_based):\n\n    M = torch.sum(mask, (1, 2))\n    res = prediction - target\n    image_loss = torch.sum(mask * res * res, (1, 2))\n\n    return reduction(image_loss, 2 * M)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:55.084665Z","iopub.execute_input":"2024-05-15T14:32:55.084943Z","iopub.status.idle":"2024-05-15T14:32:55.093588Z","shell.execute_reply.started":"2024-05-15T14:32:55.084921Z","shell.execute_reply":"2024-05-15T14:32:55.092841Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def gradient_loss(prediction, target, mask, reduction=reduction_batch_based):\n\n    M = torch.sum(mask, (1, 2))\n\n    diff = prediction - target\n    diff = torch.mul(mask, diff)\n\n    grad_x = torch.abs(diff[:, :, 1:] - diff[:, :, :-1])\n    mask_x = torch.mul(mask[:, :, 1:], mask[:, :, :-1])\n    grad_x = torch.mul(mask_x, grad_x)\n\n    grad_y = torch.abs(diff[:, 1:, :] - diff[:, :-1, :])\n    mask_y = torch.mul(mask[:, 1:, :], mask[:, :-1, :])\n    grad_y = torch.mul(mask_y, grad_y)\n\n    image_loss = torch.sum(grad_x, (1, 2)) + torch.sum(grad_y, (1, 2))\n\n    return reduction(image_loss, M)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:55.094539Z","iopub.execute_input":"2024-05-15T14:32:55.094769Z","iopub.status.idle":"2024-05-15T14:32:55.105258Z","shell.execute_reply.started":"2024-05-15T14:32:55.094749Z","shell.execute_reply":"2024-05-15T14:32:55.104464Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"class ScaleAndShiftInvariantLoss(nn.Module):\n    def __init__(self, alpha=0.5, scales=4, reduction='batch-based'):\n        super().__init__()\n\n        self.__data_loss = MSELoss(reduction=reduction)\n        self.__regularization_loss = GradientLoss(scales=scales, reduction=reduction)\n        self.__alpha = alpha\n\n        self.__prediction_ssi = None\n\n    def forward(self, prediction, target):\n        #preprocessing\n        mask = target > 0\n\n        #calcul\n        scale, shift = compute_scale_and_shift(prediction, target, mask)\n        # print(scale, shift)\n        self.__prediction_ssi = scale.view(-1, 1, 1) * prediction + shift.view(-1, 1, 1)\n\n        total = self.__data_loss(self.__prediction_ssi, target, mask)\n        if self.__alpha > 0:\n            total += self.__alpha * self.__regularization_loss(self.__prediction_ssi, target, mask)\n\n        return total\n\n    def __get_prediction_ssi(self):\n        return self.__prediction_ssi\n\n    prediction_ssi = property(__get_prediction_ssi)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:55.106134Z","iopub.execute_input":"2024-05-15T14:32:55.106365Z","iopub.status.idle":"2024-05-15T14:32:55.115727Z","shell.execute_reply.started":"2024-05-15T14:32:55.106345Z","shell.execute_reply":"2024-05-15T14:32:55.115024Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"class MSELoss(nn.Module):\n    def __init__(self, reduction='batch-based'):\n        super().__init__()\n\n        if reduction == 'batch-based':\n            self.__reduction = reduction_batch_based\n        else:\n            self.__reduction = reduction_image_based\n\n    def forward(self, prediction, target, mask):\n        return mse_loss(prediction, target, mask, reduction=self.__reduction)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:55.121357Z","iopub.execute_input":"2024-05-15T14:32:55.121709Z","iopub.status.idle":"2024-05-15T14:32:55.129373Z","shell.execute_reply.started":"2024-05-15T14:32:55.121687Z","shell.execute_reply":"2024-05-15T14:32:55.128621Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class GradientLoss(nn.Module):\n    def __init__(self, scales=4, reduction='batch-based'):\n        super().__init__()\n\n        if reduction == 'batch-based':\n            self.__reduction = reduction_batch_based\n        else:\n            self.__reduction = reduction_image_based\n\n        self.__scales = scales\n\n    def forward(self, prediction, target, mask):\n        total = 0\n\n        for scale in range(self.__scales):\n            step = pow(2, scale)\n\n            total += gradient_loss(prediction[:, ::step, ::step], target[:, ::step, ::step],\n                                   mask[:, ::step, ::step], reduction=self.__reduction)\n\n        return total","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:55.130437Z","iopub.execute_input":"2024-05-15T14:32:55.130743Z","iopub.status.idle":"2024-05-15T14:32:55.141197Z","shell.execute_reply.started":"2024-05-15T14:32:55.130720Z","shell.execute_reply":"2024-05-15T14:32:55.140365Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import os, errno\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom glob import glob\nfrom PIL import Image\nfrom torchvision import transforms, utils","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:55.142290Z","iopub.execute_input":"2024-05-15T14:32:55.142621Z","iopub.status.idle":"2024-05-15T14:32:55.155811Z","shell.execute_reply.started":"2024-05-15T14:32:55.142590Z","shell.execute_reply":"2024-05-15T14:32:55.154916Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def get_losses(config):\n    def NoneFunction(a, b):\n        return 0\n    loss_depth = NoneFunction\n    loss_segmentation = NoneFunction\n    type = config['General']['type']\n    if type == \"full\" or type==\"depth\":\n        if config['General']['loss_depth'] == 'mse':\n            loss_depth = nn.MSELoss()\n        elif config['General']['loss_depth'] == 'ssi':\n            loss_depth = ScaleAndShiftInvariantLoss()\n    if type == \"full\" or type==\"segmentation\":\n        if config['General']['loss_segmentation'] == 'ce':\n            loss_segmentation = nn.CrossEntropyLoss()\n    return loss_depth, loss_segmentation","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:55.156906Z","iopub.execute_input":"2024-05-15T14:32:55.157183Z","iopub.status.idle":"2024-05-15T14:32:55.166311Z","shell.execute_reply.started":"2024-05-15T14:32:55.157160Z","shell.execute_reply":"2024-05-15T14:32:55.165520Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def get_optimizer(config, net):\n    names = set([name.split('.')[0] for name, _ in net.named_modules()]) - set(['', 'backbone'])\n    params_backbone = net.backbone.parameters()\n    params_scratch = list()\n    for name in names:\n        params_scratch += list(eval(\"net.\"+name).parameters())\n\n    if config['General']['optim'] == 'adam':\n        optimizer_backbone = optim.Adam(params_backbone, lr=config['General']['lr_backbone'])\n        optimizer_scratch = optim.Adam(params_scratch, lr=config['General']['lr_scratch'])\n    elif config['General']['optim'] == 'sgd':\n        optimizer_backbone = optim.SGD(params_backbone, lr=config['General']['lr_backbone'], momentum=config['General']['momentum'])\n        optimizer_scratch = optim.SGD(params_scratch, lr=config['General']['lr_scratch'], momentum=config['General']['momentum'])\n    return optimizer_backbone, optimizer_scratch","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:55.167266Z","iopub.execute_input":"2024-05-15T14:32:55.167584Z","iopub.status.idle":"2024-05-15T14:32:55.177425Z","shell.execute_reply.started":"2024-05-15T14:32:55.167557Z","shell.execute_reply":"2024-05-15T14:32:55.176600Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def get_schedulers(optimizers):\n    return [ReduceLROnPlateau(optimizer) for optimizer in optimizers]","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:55.178684Z","iopub.execute_input":"2024-05-15T14:32:55.179043Z","iopub.status.idle":"2024-05-15T14:32:55.189315Z","shell.execute_reply.started":"2024-05-15T14:32:55.179010Z","shell.execute_reply":"2024-05-15T14:32:55.188427Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def create_dir(directory):\n    try:\n        os.makedirs(directory)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:55.190293Z","iopub.execute_input":"2024-05-15T14:32:55.191206Z","iopub.status.idle":"2024-05-15T14:32:55.199734Z","shell.execute_reply.started":"2024-05-15T14:32:55.191176Z","shell.execute_reply":"2024-05-15T14:32:55.199013Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"trainer","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport wandb\nimport cv2\nimport torch.nn as nn\n\nfrom tqdm import tqdm\nfrom os import replace\nfrom numpy.core.numeric import Inf","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:55.200676Z","iopub.execute_input":"2024-05-15T14:32:55.201166Z","iopub.status.idle":"2024-05-15T14:32:55.980709Z","shell.execute_reply.started":"2024-05-15T14:32:55.201143Z","shell.execute_reply":"2024-05-15T14:32:55.979730Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"class Trainer(object):\n    def __init__(self, config, pruner=None):\n        super().__init__()\n        self.config = config\n        self.type = self.config['General']['type']\n        self.pruner = pruner\n\n        self.device = torch.device(self.config['General']['device'] if torch.cuda.is_available() else \"cpu\")\n        print(\"device: %s\" % self.device)\n        resize = config['Dataset']['transforms']['resize']\n        \n        self.model = config['General']['model']\n        \n        self.loss_depth, self.loss_segmentation = get_losses(config)\n        self.optimizer_backbone, self.optimizer_scratch = get_optimizer(config, self.model)\n        self.schedulers = get_schedulers([self.optimizer_backbone, self.optimizer_scratch])\n        \n    def train(self, train_dataloader, val_dataloader):\n        epochs = self.config['General']['epochs']\n        if self.config['wandb']['enable']:\n            wandb.init(project=\"FocusOnDepth\", entity=self.config['wandb']['username'])\n            wandb.config = {\n                \"learning_rate_backbone\": self.config['General']['lr_backbone'],\n                \"learning_rate_scratch\": self.config['General']['lr_scratch'],\n                \"epochs\": epochs,\n                \"batch_size\": self.config['General']['batch_size']\n            }\n        val_loss = Inf\n        for epoch in range(epochs):  # loop over the dataset multiple times\n            print(\"Epoch \", epoch+1)\n            running_loss = 0.0\n            self.model.train()\n            pbar = tqdm(train_dataloader)\n            pbar.set_description(\"Training\")\n            for i, (X, Y_depths, Y_segmentations) in enumerate(pbar):\n                X, Y_depths, Y_segmentations = X.to(self.device), Y_depths.to(self.device), Y_segmentations.to(self.device)\n                self.optimizer_backbone.zero_grad()\n                self.optimizer_scratch.zero_grad()\n                output_depths = self.model(X)\n    \n                output_depths = output_depths['predicted_depth'].squeeze(1) if output_depths != None else None\n\n                Y_depths = Y_depths.squeeze(1) #1xHxW -> HxW\n                loss = self.loss_depth(output_depths, Y_depths)\n                loss.backward()\n                # step optimizer\n                self.optimizer_scratch.step()\n                self.optimizer_backbone.step()\n                \n                # Apply pruner to keep the model sparse\n                if self.pruner != None:\n                    self.pruner.apply(self.model)\n\n                running_loss += loss.item()\n                if np.isnan(running_loss):\n                    print('\\n',\n                        X.min().item(), X.max().item(),'\\n',\n                        Y_depths.min().item(), Y_depths.max().item(),'\\n',\n                        output_depths.min().item(), output_depths.max().item(),'\\n',\n                        loss.item(),\n                    )\n                    exit(0)\n                \n\n                if self.config['wandb']['enable'] and ((i % 50 == 0 and i>0) or i==len(train_dataloader)-1):\n                    wandb.log({\"loss\": running_loss/(i+1)})\n                pbar.set_postfix({'training_loss': running_loss/(i+1)})\n\n            new_val_loss = self.run_eval(val_dataloader)\n\n            if new_val_loss < val_loss:\n                self.save_model()\n                val_loss = new_val_loss\n\n            self.schedulers[0].step(new_val_loss)\n            self.schedulers[1].step(new_val_loss)\n\n        print('Finished Training')\n        \n    def run_eval(self, val_dataloader):\n        \"\"\"\n            Evaluate the model on the validation set and visualize some results\n            on wandb\n            :- val_dataloader -: torch dataloader\n        \"\"\"\n        val_loss = 0.\n        self.model.eval()\n        X_1 = None\n        Y_depths_1 = None\n        Y_segmentations_1 = None\n        output_depths_1 = None\n        output_segmentations_1 = None\n        with torch.no_grad():\n            pbar = tqdm(val_dataloader)\n            pbar.set_description(\"Validation\")\n            for i, (X, Y_depths, Y_segmentations) in enumerate(pbar):\n                X, Y_depths, Y_segmentations = X.to(self.device), Y_depths.to(self.device), Y_segmentations.to(self.device)\n                output_depths = self.model(X)\n                output_depths = output_depths['predicted_depth'].squeeze(1) if output_depths != None else None\n                Y_depths = Y_depths.squeeze(1)\n                if i==0:\n                    X_1 = X\n                    Y_depths_1 = Y_depths\n                    output_depths_1 = output_depths\n\n                loss = self.loss_depth(output_depths, Y_depths)\n                val_loss += loss.item()\n                pbar.set_postfix({'validation_loss': val_loss/(i+1)})\n            if self.config['wandb']['enable']:\n                wandb.log({\"val_loss\": val_loss/(i+1)})\n                self.img_logger(X_1, Y_depths_1, Y_segmentations_1, output_depths_1, output_segmentations_1)\n        return val_loss/(i+1)\n    \n    def save_model(self):\n        path_model = os.path.join(self.config['General']['path_model'], self.model.__class__.__name__)\n        create_dir(path_model)\n        torch.save(self.model.state_dict(), path_model+'.pt')\n        print('Model saved at : {}'.format(path_model))\n\n    \n    def img_logger(self, X, Y_depths, Y_segmentations, output_depths, output_segmentations):\n        nb_to_show = self.config['wandb']['images_to_show'] if self.config['wandb']['images_to_show'] <= len(X) else len(X)\n        tmp = X[:nb_to_show].detach().cpu().numpy()\n        imgs = (tmp - tmp.min()) / (tmp.max() - tmp.min())\n        if output_depths != None:\n            tmp = Y_depths[:nb_to_show].unsqueeze(1).detach().cpu().numpy()\n            depth_truths = np.repeat(tmp, 3, axis=1)\n            tmp = output_depths[:nb_to_show].unsqueeze(1).detach().cpu().numpy()\n            tmp = np.repeat(tmp, 3, axis=1)\n            #depth_preds = 1.0 - tmp\n            depth_preds = tmp\n        if output_segmentations != None:\n            tmp = Y_segmentations[:nb_to_show].unsqueeze(1).detach().cpu().numpy()\n            segmentation_truths = np.repeat(tmp, 3, axis=1).astype('float32')\n            tmp = torch.argmax(output_segmentations[:nb_to_show], dim=1)\n            tmp = tmp.unsqueeze(1).detach().cpu().numpy()\n            tmp = np.repeat(tmp, 3, axis=1)\n            segmentation_preds = tmp.astype('float32')\n        imgs = imgs.transpose(0,2,3,1)\n        if output_depths != None:\n            depth_truths = depth_truths.transpose(0,2,3,1)\n            depth_preds = depth_preds.transpose(0,2,3,1)\n        if output_segmentations != None:\n            segmentation_truths = segmentation_truths.transpose(0,2,3,1)\n            segmentation_preds = segmentation_preds.transpose(0,2,3,1)\n        output_dim = (int(self.config['wandb']['im_w']), int(self.config['wandb']['im_h']))\n\n        wandb.log({\n            \"img\": [wandb.Image(cv2.resize(im, output_dim), caption='img_{}'.format(i+1)) for i, im in enumerate(imgs)]\n        })\n        if output_depths != None:\n            wandb.log({\n                \"depth_truths\": [wandb.Image(cv2.resize(im, output_dim), caption='depth_truths_{}'.format(i+1)) for i, im in enumerate(depth_truths)],\n                \"depth_preds\": [wandb.Image(cv2.resize(im, output_dim), caption='depth_preds_{}'.format(i+1)) for i, im in enumerate(depth_preds)]\n            })\n        if output_segmentations != None:\n            wandb.log({\n                \"seg_truths\": [wandb.Image(cv2.resize(im, output_dim), caption='seg_truths_{}'.format(i+1)) for i, im in enumerate(segmentation_truths)],\n                \"seg_preds\": [wandb.Image(cv2.resize(im, output_dim), caption='seg_preds_{}'.format(i+1)) for i, im in enumerate(segmentation_preds)]\n            })","metadata":{"execution":{"iopub.status.busy":"2024-05-15T15:34:10.424904Z","iopub.execute_input":"2024-05-15T15:34:10.425307Z","iopub.status.idle":"2024-05-15T15:34:10.463939Z","shell.execute_reply.started":"2024-05-15T15:34:10.425274Z","shell.execute_reply":"2024-05-15T15:34:10.462946Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"def eval_fn(model, val_dataloader):\n        \"\"\"\n            Evaluate the model on the validation set and visualize some results\n            on wandb\n            :- model -: torch model using for evaluate\n            :- val_dataloader -: torch dataloader\n        \"\"\"\n        loss_depth, loss_segmentation = get_losses(config)\n        device = torch.device(config['General']['device'] if torch.cuda.is_available() else \"cpu\")\n        \n        val_loss = 0.\n        model.eval()\n        \n        #\n        X_1 = None \n        Y_depths_1 = None\n        output_depths_1 = None\n        \n        #\n        with torch.no_grad():\n            for i, (X, Y_depths, _) in enumerate(val_dataloader):\n                X, Y_depths = X.to(device), Y_depths.to(device)\n                output_depths = model(X)\n                output_depths = output_depths['predicted_depth'].squeeze(1) if output_depths != None else None\n                Y_depths = Y_depths.squeeze(1)\n                if i==0:\n                    X_1 = X\n                    Y_depths_1 = Y_depths\n                    output_depths_1 = output_depths\n                    \n                # get loss\n                loss = loss_depth(output_depths, Y_depths)\n                val_loss += loss.item()\n#                 pbar.set_postfix({'validation_loss': val_loss/(i+1)})\n        return val_loss/(i+1)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:56.021012Z","iopub.execute_input":"2024-05-15T14:32:56.021423Z","iopub.status.idle":"2024-05-15T14:32:56.034073Z","shell.execute_reply.started":"2024-05-15T14:32:56.021392Z","shell.execute_reply":"2024-05-15T14:32:56.033290Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"model_path = \"/kaggle/input/weight-dpt-swin2-tiny-256-ssiloss/models/DPTForDepthEstimation-ssi.pt\"\n\ncheckpoint = torch.load(model_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:56.035066Z","iopub.execute_input":"2024-05-15T14:32:56.035327Z","iopub.status.idle":"2024-05-15T14:32:58.563145Z","shell.execute_reply.started":"2024-05-15T14:32:56.035305Z","shell.execute_reply":"2024-05-15T14:32:58.562181Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:58.564488Z","iopub.execute_input":"2024-05-15T14:32:58.564821Z","iopub.status.idle":"2024-05-15T14:32:58.587627Z","shell.execute_reply.started":"2024-05-15T14:32:58.564795Z","shell.execute_reply":"2024-05-15T14:32:58.586393Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:58.589165Z","iopub.execute_input":"2024-05-15T14:32:58.589848Z","iopub.status.idle":"2024-05-15T14:32:58.604774Z","shell.execute_reply.started":"2024-05-15T14:32:58.589810Z","shell.execute_reply":"2024-05-15T14:32:58.603784Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"DPTForDepthEstimation(\n  (backbone): Swinv2Backbone(\n    (embeddings): Swinv2Embeddings(\n      (patch_embeddings): Swinv2PatchEmbeddings(\n        (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n      )\n      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): Swinv2Encoder(\n      (layers): ModuleList(\n        (0): Swinv2Stage(\n          (blocks): ModuleList(\n            (0-1): 2 x Swinv2Layer(\n              (attention): Swinv2Attention(\n                (self): Swinv2SelfAttention(\n                  (continuous_position_bias_mlp): Sequential(\n                    (0): Linear(in_features=2, out_features=512, bias=True)\n                    (1): ReLU(inplace=True)\n                    (2): Linear(in_features=512, out_features=3, bias=False)\n                  )\n                  (query): Linear(in_features=96, out_features=96, bias=True)\n                  (key): Linear(in_features=96, out_features=96, bias=False)\n                  (value): Linear(in_features=96, out_features=96, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): Swinv2SelfOutput(\n                  (dense): Linear(in_features=96, out_features=96, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n              (drop_path): Swinv2DropPath(p=0.1)\n              (intermediate): Swinv2Intermediate(\n                (dense): Linear(in_features=96, out_features=384, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): Swinv2Output(\n                (dense): Linear(in_features=384, out_features=96, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n          (downsample): Swinv2PatchMerging(\n            (reduction): Linear(in_features=384, out_features=192, bias=False)\n            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (1): Swinv2Stage(\n          (blocks): ModuleList(\n            (0-1): 2 x Swinv2Layer(\n              (attention): Swinv2Attention(\n                (self): Swinv2SelfAttention(\n                  (continuous_position_bias_mlp): Sequential(\n                    (0): Linear(in_features=2, out_features=512, bias=True)\n                    (1): ReLU(inplace=True)\n                    (2): Linear(in_features=512, out_features=6, bias=False)\n                  )\n                  (query): Linear(in_features=192, out_features=192, bias=True)\n                  (key): Linear(in_features=192, out_features=192, bias=False)\n                  (value): Linear(in_features=192, out_features=192, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): Swinv2SelfOutput(\n                  (dense): Linear(in_features=192, out_features=192, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n              (drop_path): Swinv2DropPath(p=0.1)\n              (intermediate): Swinv2Intermediate(\n                (dense): Linear(in_features=192, out_features=768, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): Swinv2Output(\n                (dense): Linear(in_features=768, out_features=192, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n          (downsample): Swinv2PatchMerging(\n            (reduction): Linear(in_features=768, out_features=384, bias=False)\n            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (2): Swinv2Stage(\n          (blocks): ModuleList(\n            (0-5): 6 x Swinv2Layer(\n              (attention): Swinv2Attention(\n                (self): Swinv2SelfAttention(\n                  (continuous_position_bias_mlp): Sequential(\n                    (0): Linear(in_features=2, out_features=512, bias=True)\n                    (1): ReLU(inplace=True)\n                    (2): Linear(in_features=512, out_features=12, bias=False)\n                  )\n                  (query): Linear(in_features=384, out_features=384, bias=True)\n                  (key): Linear(in_features=384, out_features=384, bias=False)\n                  (value): Linear(in_features=384, out_features=384, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): Swinv2SelfOutput(\n                  (dense): Linear(in_features=384, out_features=384, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (drop_path): Swinv2DropPath(p=0.1)\n              (intermediate): Swinv2Intermediate(\n                (dense): Linear(in_features=384, out_features=1536, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): Swinv2Output(\n                (dense): Linear(in_features=1536, out_features=384, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n          (downsample): Swinv2PatchMerging(\n            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (3): Swinv2Stage(\n          (blocks): ModuleList(\n            (0-1): 2 x Swinv2Layer(\n              (attention): Swinv2Attention(\n                (self): Swinv2SelfAttention(\n                  (continuous_position_bias_mlp): Sequential(\n                    (0): Linear(in_features=2, out_features=512, bias=True)\n                    (1): ReLU(inplace=True)\n                    (2): Linear(in_features=512, out_features=24, bias=False)\n                  )\n                  (query): Linear(in_features=768, out_features=768, bias=True)\n                  (key): Linear(in_features=768, out_features=768, bias=False)\n                  (value): Linear(in_features=768, out_features=768, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): Swinv2SelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (drop_path): Swinv2DropPath(p=0.1)\n              (intermediate): Swinv2Intermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): Swinv2Output(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n      )\n    )\n  )\n  (neck): DPTNeck(\n    (convs): ModuleList(\n      (0): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (2): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (3): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    )\n    (fusion_stage): DPTFeatureFusionStage(\n      (layers): ModuleList(\n        (0-3): 4 x DPTFeatureFusionLayer(\n          (projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n          (residual_layer1): DPTPreActResidualLayer(\n            (activation1): ReLU()\n            (convolution1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (activation2): ReLU()\n            (convolution2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n          (residual_layer2): DPTPreActResidualLayer(\n            (activation1): ReLU()\n            (convolution1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (activation2): ReLU()\n            (convolution2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n      )\n    )\n  )\n  (head): DPTDepthEstimationHead(\n    (head): Sequential(\n      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): Upsample(scale_factor=2.0, mode='bilinear')\n      (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): ReLU()\n      (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n      (5): ReLU()\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"recover_model = lambda : model.load_state_dict(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:58.605926Z","iopub.execute_input":"2024-05-15T14:32:58.606219Z","iopub.status.idle":"2024-05-15T14:32:58.613985Z","shell.execute_reply.started":"2024-05-15T14:32:58.606195Z","shell.execute_reply":"2024-05-15T14:32:58.613069Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"Helper Functions (Flops, Model Size calculation, etc.)","metadata":{}},{"cell_type":"code","source":"def get_model_macs(model, inputs) -> int:\n    return profile_macs(model, inputs)\n\n\ndef get_sparsity(tensor: torch.Tensor) -> float:\n    \"\"\"\n    calculate the sparsity of the given tensor\n        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n    \"\"\"\n    return 1 - float(tensor.count_nonzero()) / tensor.numel()\n\n\ndef get_model_sparsity(model: nn.Module) -> float:\n    \"\"\"\n    calculate the sparsity of the given model\n        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n    \"\"\"\n    num_nonzeros, num_elements = 0, 0\n    for param in model.parameters():\n        num_nonzeros += param.count_nonzero()\n        num_elements += param.numel()\n    return 1 - float(num_nonzeros) / num_elements\n\ndef get_num_parameters(model: nn.Module, count_nonzero_only=False) -> int:\n    \"\"\"\n    calculate the total number of parameters of model\n    :param count_nonzero_only: only count nonzero weights\n    \"\"\"\n    num_counted_elements = 0\n    for param in model.parameters():\n        if count_nonzero_only:\n            num_counted_elements += param.count_nonzero()\n        else:\n            num_counted_elements += param.numel()\n    return num_counted_elements\n\n\ndef get_model_size(model: nn.Module, data_width=32, count_nonzero_only=False) -> int:\n    \"\"\"\n    calculate the model size in bits\n    :param data_width: #bits per element\n    :param count_nonzero_only: only count nonzero weights\n    \"\"\"\n    return get_num_parameters(model, count_nonzero_only) * data_width\n\nByte = 8\nKiB = 1024 * Byte\nMiB = 1024 * KiB\nGiB = 1024 * MiB","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:58.615160Z","iopub.execute_input":"2024-05-15T14:32:58.615495Z","iopub.status.idle":"2024-05-15T14:32:58.625572Z","shell.execute_reply.started":"2024-05-15T14:32:58.615464Z","shell.execute_reply":"2024-05-15T14:32:58.624443Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"dense_model_accuracy = eval_fn(model, test_dataloader)\ndense_model_size = get_model_size(model)\nprint(f\"dense model has ssi loss={dense_model_accuracy:.2f}\")\nprint(f\"dense model has size={dense_model_size/MiB:.2f} MiB\")","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:32:58.626631Z","iopub.execute_input":"2024-05-15T14:32:58.626971Z","iopub.status.idle":"2024-05-15T14:35:23.946703Z","shell.execute_reply.started":"2024-05-15T14:32:58.626934Z","shell.execute_reply":"2024-05-15T14:35:23.945675Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"dense model has ssi loss=0.03\ndense model has size=156.14 MiB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# fine_grained_prune","metadata":{}},{"cell_type":"code","source":"def fine_grained_prune(tensor: torch.Tensor, sparsity : float) -> torch.Tensor:\n    \"\"\"\n    magnitude-based pruning for single tensor\n    :param tensor: torch.(cuda.)Tensor, weight of conv/fc layer\n    :param sparsity: float, pruning sparsity\n        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n    :return:\n        torch.(cuda.)Tensor, mask for zeros\n    \"\"\"\n    sparsity = min(max(0.0, sparsity), 1.0)\n    if sparsity == 1.0:\n        tensor.zero_()\n        return torch.zeros_like(tensor)\n    elif sparsity == 0.0:\n        return torch.ones_like(tensor)\n    \n    num_elements = tensor.numel()\n    #\n    # Step 1: calculate the #zeros (please use round())\n    num_zeros = round(num_elements * sparsity)\n    \n    # Step 2: calculate the importance of weight\n    importance = torch.abs(tensor)\n    \n    # Step 3: calculate the pruning threshold\n    threshold = torch.kthvalue(importance.flatten(), num_zeros)[0]\n    \n    # Step 4: get binary mask (1 for nonzeros, 0 for zeros)\n    mask = importance > threshold\n    #\n    tensor.mul_(mask)\n\n    return mask","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:35:23.947926Z","iopub.execute_input":"2024-05-15T14:35:23.948185Z","iopub.status.idle":"2024-05-15T14:35:23.955650Z","shell.execute_reply.started":"2024-05-15T14:35:23.948163Z","shell.execute_reply":"2024-05-15T14:35:23.954672Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"class FineGrainedPruner:\n    def __init__(self, model, sparsity_dict):\n        self.masks = FineGrainedPruner.prune(model, sparsity_dict)\n\n    @torch.no_grad()\n    def apply(self, model):\n        for name, param in model.named_parameters():\n            if name in self.masks:\n                param *= self.masks[name]\n\n    @staticmethod\n    @torch.no_grad()\n    def prune(model, sparsity_dict):\n        masks = dict()\n        for name, param in model.named_parameters():\n            if param.dim() > 3: # we only prune conv and fc weights\n                masks[name] = fine_grained_prune(param, sparsity_dict[name])\n        return masks","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:35:23.957066Z","iopub.execute_input":"2024-05-15T14:35:23.957680Z","iopub.status.idle":"2024-05-15T14:35:23.974497Z","shell.execute_reply.started":"2024-05-15T14:35:23.957649Z","shell.execute_reply":"2024-05-15T14:35:23.973619Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"sparsity dict","metadata":{}},{"cell_type":"code","source":"sparsity_dict = {\n    'backbone.embeddings.patch_embeddings.projection.weight': 0,\n    'neck.convs.0.weight':0.9,\n    'neck.convs.1.weight':0.9,\n    'neck.convs.2.weight':0.9,\n    'neck.convs.3.weight':0.8,\n    'neck.fusion_stage.layers.0.projection.weight':0.8,\n    'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight':0.9,\n    'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight':0.9,\n    'neck.fusion_stage.layers.0.residual_layer2.convolution1.weight':0.7,\n    'neck.fusion_stage.layers.0.residual_layer2.convolution2.weight':0.9,\n    'neck.fusion_stage.layers.1.projection.weight':0.7,\n    'neck.fusion_stage.layers.1.residual_layer1.convolution1.weight':0.9,\n    'neck.fusion_stage.layers.1.residual_layer1.convolution2.weight':0.8,\n    'neck.fusion_stage.layers.1.residual_layer2.convolution1.weight':0.9,\n    'neck.fusion_stage.layers.1.residual_layer2.convolution2.weight':0.9,\n    'neck.fusion_stage.layers.2.projection.weight':0.9,\n    'neck.fusion_stage.layers.2.residual_layer1.convolution1.weight':0.9,\n    'neck.fusion_stage.layers.2.residual_layer1.convolution2.weight':0.7,\n    'neck.fusion_stage.layers.2.residual_layer2.convolution1.weight':0.9,\n    'neck.fusion_stage.layers.2.residual_layer2.convolution2.weight':0.9,\n    'neck.fusion_stage.layers.3.projection.weight':0.9,\n    'neck.fusion_stage.layers.3.residual_layer1.convolution1.weight':0.8,\n    'neck.fusion_stage.layers.3.residual_layer1.convolution2.weight':0.9,\n    'neck.fusion_stage.layers.3.residual_layer2.convolution1.weight':0.9,\n    'neck.fusion_stage.layers.3.residual_layer2.convolution2.weight':0.6,\n    'head.head.0.weight':0.7,\n    'head.head.2.weight':0.7,\n    'head.head.4.weight':0.6\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:35:23.975396Z","iopub.execute_input":"2024-05-15T14:35:23.975678Z","iopub.status.idle":"2024-05-15T14:35:23.987749Z","shell.execute_reply.started":"2024-05-15T14:35:23.975657Z","shell.execute_reply":"2024-05-15T14:35:23.986835Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"pruner = FineGrainedPruner(model, sparsity_dict)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:35:23.988670Z","iopub.execute_input":"2024-05-15T14:35:23.988958Z","iopub.status.idle":"2024-05-15T14:35:24.021609Z","shell.execute_reply.started":"2024-05-15T14:35:23.988936Z","shell.execute_reply":"2024-05-15T14:35:24.020931Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"print(f'After pruning with sparsity dictionary')\nfor name, sparsity in sparsity_dict.items():\n    print(f'  {name}: {sparsity:.2f}')\nprint(f'The sparsity of each layer becomes')\nfor name, param in model.named_parameters():\n    if name in sparsity_dict:\n        print(f'  {name}: {get_sparsity(param):.2f}')\n\nsparse_model_size = get_model_size(model, count_nonzero_only=True)\nprint(f\"Sparse model has size={sparse_model_size / MiB:.2f} MiB = {sparse_model_size / dense_model_size * 100:.2f}% of dense model size\")\nsparse_model_accuracy = eval_fn(model, test_dataloader)\nprint(f\"Sparse model has ssi loss ={sparse_model_accuracy:.2f} before fintuning\")","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:35:24.022621Z","iopub.execute_input":"2024-05-15T14:35:24.022945Z","iopub.status.idle":"2024-05-15T14:37:12.850163Z","shell.execute_reply.started":"2024-05-15T14:35:24.022917Z","shell.execute_reply":"2024-05-15T14:37:12.849190Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"After pruning with sparsity dictionary\n  backbone.embeddings.patch_embeddings.projection.weight: 0.00\n  neck.convs.0.weight: 0.90\n  neck.convs.1.weight: 0.90\n  neck.convs.2.weight: 0.90\n  neck.convs.3.weight: 0.80\n  neck.fusion_stage.layers.0.projection.weight: 0.80\n  neck.fusion_stage.layers.0.residual_layer1.convolution1.weight: 0.90\n  neck.fusion_stage.layers.0.residual_layer1.convolution2.weight: 0.90\n  neck.fusion_stage.layers.0.residual_layer2.convolution1.weight: 0.70\n  neck.fusion_stage.layers.0.residual_layer2.convolution2.weight: 0.90\n  neck.fusion_stage.layers.1.projection.weight: 0.70\n  neck.fusion_stage.layers.1.residual_layer1.convolution1.weight: 0.90\n  neck.fusion_stage.layers.1.residual_layer1.convolution2.weight: 0.80\n  neck.fusion_stage.layers.1.residual_layer2.convolution1.weight: 0.90\n  neck.fusion_stage.layers.1.residual_layer2.convolution2.weight: 0.90\n  neck.fusion_stage.layers.2.projection.weight: 0.90\n  neck.fusion_stage.layers.2.residual_layer1.convolution1.weight: 0.90\n  neck.fusion_stage.layers.2.residual_layer1.convolution2.weight: 0.70\n  neck.fusion_stage.layers.2.residual_layer2.convolution1.weight: 0.90\n  neck.fusion_stage.layers.2.residual_layer2.convolution2.weight: 0.90\n  neck.fusion_stage.layers.3.projection.weight: 0.90\n  neck.fusion_stage.layers.3.residual_layer1.convolution1.weight: 0.80\n  neck.fusion_stage.layers.3.residual_layer1.convolution2.weight: 0.90\n  neck.fusion_stage.layers.3.residual_layer2.convolution1.weight: 0.90\n  neck.fusion_stage.layers.3.residual_layer2.convolution2.weight: 0.60\n  head.head.0.weight: 0.70\n  head.head.2.weight: 0.70\n  head.head.4.weight: 0.60\nThe sparsity of each layer becomes\n  backbone.embeddings.patch_embeddings.projection.weight: 0.00\n  neck.convs.0.weight: 0.90\n  neck.convs.1.weight: 0.90\n  neck.convs.2.weight: 0.90\n  neck.convs.3.weight: 0.80\n  neck.fusion_stage.layers.0.projection.weight: 0.80\n  neck.fusion_stage.layers.0.residual_layer1.convolution1.weight: 0.90\n  neck.fusion_stage.layers.0.residual_layer1.convolution2.weight: 0.90\n  neck.fusion_stage.layers.0.residual_layer2.convolution1.weight: 0.70\n  neck.fusion_stage.layers.0.residual_layer2.convolution2.weight: 0.90\n  neck.fusion_stage.layers.1.projection.weight: 0.70\n  neck.fusion_stage.layers.1.residual_layer1.convolution1.weight: 0.90\n  neck.fusion_stage.layers.1.residual_layer1.convolution2.weight: 0.80\n  neck.fusion_stage.layers.1.residual_layer2.convolution1.weight: 0.90\n  neck.fusion_stage.layers.1.residual_layer2.convolution2.weight: 0.90\n  neck.fusion_stage.layers.2.projection.weight: 0.90\n  neck.fusion_stage.layers.2.residual_layer1.convolution1.weight: 0.90\n  neck.fusion_stage.layers.2.residual_layer1.convolution2.weight: 0.70\n  neck.fusion_stage.layers.2.residual_layer2.convolution1.weight: 0.90\n  neck.fusion_stage.layers.2.residual_layer2.convolution2.weight: 0.90\n  neck.fusion_stage.layers.3.projection.weight: 0.90\n  neck.fusion_stage.layers.3.residual_layer1.convolution1.weight: 0.80\n  neck.fusion_stage.layers.3.residual_layer1.convolution2.weight: 0.90\n  neck.fusion_stage.layers.3.residual_layer2.convolution1.weight: 0.90\n  neck.fusion_stage.layers.3.residual_layer2.convolution2.weight: 0.60\n  head.head.0.weight: 0.70\n  head.head.2.weight: 0.70\n  head.head.4.weight: 0.59\nSparse model has size=113.34 MiB = 72.59% of dense model size\nSparse model has ssi loss =0.09 before fintuning\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Finetune the fine-grained pruned model","metadata":{}},{"cell_type":"code","source":"pruner.masks.keys()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:37:12.851302Z","iopub.execute_input":"2024-05-15T14:37:12.851559Z","iopub.status.idle":"2024-05-15T14:37:12.858360Z","shell.execute_reply.started":"2024-05-15T14:37:12.851536Z","shell.execute_reply":"2024-05-15T14:37:12.857484Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"dict_keys(['backbone.embeddings.patch_embeddings.projection.weight', 'neck.convs.0.weight', 'neck.convs.1.weight', 'neck.convs.2.weight', 'neck.convs.3.weight', 'neck.fusion_stage.layers.0.projection.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer2.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer2.convolution2.weight', 'neck.fusion_stage.layers.1.projection.weight', 'neck.fusion_stage.layers.1.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.1.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.1.residual_layer2.convolution1.weight', 'neck.fusion_stage.layers.1.residual_layer2.convolution2.weight', 'neck.fusion_stage.layers.2.projection.weight', 'neck.fusion_stage.layers.2.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.2.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.2.residual_layer2.convolution1.weight', 'neck.fusion_stage.layers.2.residual_layer2.convolution2.weight', 'neck.fusion_stage.layers.3.projection.weight', 'neck.fusion_stage.layers.3.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.3.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.3.residual_layer2.convolution1.weight', 'neck.fusion_stage.layers.3.residual_layer2.convolution2.weight', 'head.head.0.weight', 'head.head.2.weight', 'head.head.4.weight'])"},"metadata":{}}]},{"cell_type":"code","source":"trainer = Trainer(config, pruner)\ntrainer.train(train_dataloader, val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:37:12.859398Z","iopub.execute_input":"2024-05-15T14:37:12.859693Z","iopub.status.idle":"2024-05-15T14:49:13.655944Z","shell.execute_reply.started":"2024-05-15T14:37:12.859664Z","shell.execute_reply":"2024-05-15T14:49:13.654928Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"device: cuda\nEpoch  1\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 1512/1512 [03:18<00:00,  7.61it/s, training_loss=0.0378]\nValidation: 100%|| 504/504 [00:48<00:00, 10.44it/s, validation_loss=0.0334]\n","output_type":"stream"},{"name":"stdout","text":"Model saved at : models/DPTForDepthEstimation\nEpoch  2\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 1512/1512 [03:19<00:00,  7.57it/s, training_loss=0.0318]\nValidation: 100%|| 504/504 [00:37<00:00, 13.34it/s, validation_loss=0.0322]\n","output_type":"stream"},{"name":"stdout","text":"Model saved at : models/DPTForDepthEstimation\nEpoch  3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 1512/1512 [03:18<00:00,  7.63it/s, training_loss=0.0305]\nValidation: 100%|| 504/504 [00:37<00:00, 13.58it/s, validation_loss=0.0307]\n","output_type":"stream"},{"name":"stdout","text":"Model saved at : models/DPTForDepthEstimation\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"sparse_model_size = get_model_size(model, count_nonzero_only=True)\nprint(f\"Sparse model has size={sparse_model_size / MiB:.2f} MiB = {sparse_model_size / dense_model_size * 100:.2f}% of dense model size\")\nsparse_model_accuracy = eval_fn(model, test_dataloader)\nprint(f\"Sparse model has ssi loss={sparse_model_accuracy:.2f}% after fintuning\")","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:49:13.657666Z","iopub.execute_input":"2024-05-15T14:49:13.657992Z","iopub.status.idle":"2024-05-15T14:51:04.659001Z","shell.execute_reply.started":"2024-05-15T14:49:13.657966Z","shell.execute_reply":"2024-05-15T14:51:04.658045Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Sparse model has size=113.34 MiB = 72.59% of dense model size\nSparse model has ssi loss=0.03% after fintuning\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Quantization","metadata":{}},{"cell_type":"code","source":"!pip install -qqq fast-pytorch-kmeans","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:55:14.321531Z","iopub.execute_input":"2024-05-15T14:55:14.321952Z","iopub.status.idle":"2024-05-15T14:55:26.743454Z","shell.execute_reply.started":"2024-05-15T14:55:14.321921Z","shell.execute_reply":"2024-05-15T14:55:26.742019Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"import copy ","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:55:32.020104Z","iopub.execute_input":"2024-05-15T14:55:32.021071Z","iopub.status.idle":"2024-05-15T14:55:32.025728Z","shell.execute_reply.started":"2024-05-15T14:55:32.021033Z","shell.execute_reply":"2024-05-15T14:55:32.024571Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"from collections import namedtuple\n    \nCodebook = namedtuple('Codebook', ['centroids', 'labels'])","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:55:33.021252Z","iopub.execute_input":"2024-05-15T14:55:33.022095Z","iopub.status.idle":"2024-05-15T14:55:33.026745Z","shell.execute_reply.started":"2024-05-15T14:55:33.022061Z","shell.execute_reply":"2024-05-15T14:55:33.025713Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"from fast_pytorch_kmeans import KMeans","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:55:33.543812Z","iopub.execute_input":"2024-05-15T14:55:33.544214Z","iopub.status.idle":"2024-05-15T14:55:33.554349Z","shell.execute_reply.started":"2024-05-15T14:55:33.544186Z","shell.execute_reply":"2024-05-15T14:55:33.553358Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"def k_means_quantize(fp32_tensor: torch.Tensor, bitwidth=4, codebook=None):\n    \"\"\"\n    quantize tensor using k-means clustering\n    :param fp32_tensor:\n    :param bitwidth: [int] quantization bit width, default=4\n    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)\n    :return:\n        [Codebook = (centroids, labels)]\n            centroids: [torch.(cuda.)FloatTensor] the cluster centroids\n            labels: [torch.(cuda.)LongTensor] cluster label tensor\n    \"\"\"\n    if codebook is None:\n        n_clusters = 2**bitwidth\n        kmeans = KMeans(n_clusters=n_clusters, mode='euclidean', verbose=0)\n        labels = kmeans.fit_predict(fp32_tensor.view(-1, 1)).to(torch.long)\n        centroids = kmeans.centroids.to(torch.float).view(-1)\n        codebook = Codebook(centroids, labels)\n    quantized_tensor = codebook.centroids[codebook.labels]\n    fp32_tensor.set_(quantized_tensor.view_as(fp32_tensor))\n    return codebook","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:55:34.722242Z","iopub.execute_input":"2024-05-15T14:55:34.722605Z","iopub.status.idle":"2024-05-15T14:55:34.730013Z","shell.execute_reply.started":"2024-05-15T14:55:34.722577Z","shell.execute_reply":"2024-05-15T14:55:34.729020Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"Quantize in whole model","metadata":{}},{"cell_type":"code","source":"class KMeansQuantizer:\n    def __init__(self, model : nn.Module, bitwidth=4):\n        self.codebook = KMeansQuantizer.quantize(model, bitwidth)\n        \n    @torch.no_grad()\n    def apply(self, model, update_centroids):\n        for name, param in model.named_parameters():\n            if name in self.codebook:\n                if update_centroids:\n                    update_codebook(param, codebook=self.codebook[name])\n                self.codebook[name] = k_means_quantize(\n                    param, codebook=self.codebook[name])\n                \n    @staticmethod\n    @torch.no_grad()\n    def quantize(model: nn.Module, bitwidth=4):\n        codebook = dict()\n        if isinstance(bitwidth, dict):\n            for name, param in model.named_parameters():\n                if name in bitwidth:\n                    codebook[name] = k_means_quantize(param, bitwidth=bitwidth[name])\n        else:\n            for name, param in model.named_parameters():\n                # print(f\"{name=}: {param.shape}\")\n                # only quantize weight, not bias\n                if param.dim() > 1:\n                    codebook[name] = k_means_quantize(param, bitwidth=bitwidth)\n        return codebook","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:55:37.220073Z","iopub.execute_input":"2024-05-15T14:55:37.220451Z","iopub.status.idle":"2024-05-15T14:55:37.231033Z","shell.execute_reply.started":"2024-05-15T14:55:37.220422Z","shell.execute_reply":"2024-05-15T14:55:37.230098Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"quantizers = dict()\n\nfor bitwidth in [8, 4, 2]:\n    # Recover model\n    model_copy = copy.deepcopy(model)\n    print(f'k-means quantizing model into {bitwidth} bits')\n    quantizer = KMeansQuantizer(model_copy, bitwidth) # codebook\n    quantized_model_size = get_model_size(model_copy, bitwidth, True)\n    print(f\"    {bitwidth}-bit k-means quantized model has size={quantized_model_size/MiB:.2f} MiB\")\n    quantized_model_accuracy = eval_fn(model_copy, test_dataloader)\n    print(f\"    {bitwidth}-bit k-means quantized model has ssi loss={quantized_model_accuracy:.2f}\")\n    quantizers[bitwidth] = quantizer","metadata":{"execution":{"iopub.status.busy":"2024-05-15T15:20:52.256470Z","iopub.execute_input":"2024-05-15T15:20:52.257159Z","iopub.status.idle":"2024-05-15T15:26:55.062554Z","shell.execute_reply.started":"2024-05-15T15:20:52.257118Z","shell.execute_reply":"2024-05-15T15:26:55.061563Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"k-means quantizing model into 8 bits\n    8-bit k-means quantized model has size=37.86 MiB\n    8-bit k-means quantized model has ssi loss=0.03\nk-means quantizing model into 4 bits\n    4-bit k-means quantized model has size=19.39 MiB\n    4-bit k-means quantized model has ssi loss=0.08\nk-means quantizing model into 2 bits\n    2-bit k-means quantized model has size=9.76 MiB\n    2-bit k-means quantized model has ssi loss=0.11\n","output_type":"stream"}]},{"cell_type":"code","source":"quantizers","metadata":{"execution":{"iopub.status.busy":"2024-05-15T15:30:43.561649Z","iopub.execute_input":"2024-05-15T15:30:43.562129Z","iopub.status.idle":"2024-05-15T15:30:43.568230Z","shell.execute_reply.started":"2024-05-15T15:30:43.562095Z","shell.execute_reply":"2024-05-15T15:30:43.567338Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"{8: <__main__.KMeansQuantizer at 0x7c205fba1420>,\n 4: <__main__.KMeansQuantizer at 0x7c20b84692d0>,\n 2: <__main__.KMeansQuantizer at 0x7c206c4d4340>}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Trained K Mean quantization","metadata":{}},{"cell_type":"code","source":"def update_codebook(fp32_tensor: torch.Tensor, codebook: Codebook):\n    \"\"\"\n    update the centroids in the codebook using updated fp32_tensor\n    :param fp32_tensor: [torch.(cuda.)Tensor]\n    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)\n    \"\"\"\n    n_clusters = codebook.centroids.numel()\n    fp32_tensor = fp32_tensor.view(-1)\n    for k in range(n_clusters):\n        codebook.centroids[k] = torch.mean(fp32_tensor[codebook.labels==k])","metadata":{"execution":{"iopub.status.busy":"2024-05-15T15:27:08.658572Z","iopub.execute_input":"2024-05-15T15:27:08.658977Z","iopub.status.idle":"2024-05-15T15:27:08.664682Z","shell.execute_reply.started":"2024-05-15T15:27:08.658945Z","shell.execute_reply":"2024-05-15T15:27:08.663703Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"fp32_model_accuracy = eval_fn(model, test_dataloader)\nprint(f\"fp32 model has ssi loss={fp32_model_accuracy:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-15T15:27:11.353188Z","iopub.execute_input":"2024-05-15T15:27:11.353529Z","iopub.status.idle":"2024-05-15T15:29:01.808836Z","shell.execute_reply.started":"2024-05-15T15:27:11.353505Z","shell.execute_reply":"2024-05-15T15:29:01.807825Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"fp32 model has ssi loss=0.03\n","output_type":"stream"}]},{"cell_type":"code","source":"accuracy_drop_threshold = 0.02\nquantizers_before_finetune = copy.deepcopy(quantizers)\nquantizers_after_finetune = quantizers","metadata":{"execution":{"iopub.status.busy":"2024-05-15T15:31:36.209747Z","iopub.execute_input":"2024-05-15T15:31:36.210153Z","iopub.status.idle":"2024-05-15T15:31:36.292626Z","shell.execute_reply.started":"2024-05-15T15:31:36.210123Z","shell.execute_reply":"2024-05-15T15:31:36.291863Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"for bitwidth in [8, 4, 2]:\n    model_copy = copy.deepcopy(model)\n    quantizer = quantizers[bitwidth]\n    print(f'k-means quantizing model into {bitwidth} bits')\n    quantizer.apply(model_copy, update_centroids=False)\n    quantized_model_size = get_model_size(model_copy, bitwidth, True)\n    print(f\"    {bitwidth}-bit k-means quantized model has size={quantized_model_size/MiB:.2f} MiB\")\n    quantized_model_accuracy = eval_fn(model_copy, test_dataloader)\n    print(f\"    {bitwidth}-bit k-means quantized model has ssi loss={quantized_model_accuracy:.2f} before quantization-aware training \")\n    accuracy_drop = abs(fp32_model_accuracy - quantized_model_accuracy)\n    if accuracy_drop > accuracy_drop_threshold:\n        print(f\"        Quantization-aware training due to ssi loss drop={accuracy_drop:.2f} is larger than threshold={accuracy_drop_threshold:.2f}\")\n        config[\"General\"][\"model\"] = model_copy\n        trainer = Trainer(config)\n        trainer.train(train_dataloader, val_dataloader)\n        quantized_model_accuracy = eval_fn(model_copy, test_dataloader)\n        print(f\"    {bitwidth}-bit k-means quantized model has ssi loss={quantized_model_accuracy:.2f} before quantization-aware training \")\n    else:\n        print(f\"        No need for quantization-aware training since ssi loss drop={accuracy_drop:.2f} is smaller than threshold={accuracy_drop_threshold:.2f}\")\n     ","metadata":{"execution":{"iopub.status.busy":"2024-05-15T15:34:19.964580Z","iopub.execute_input":"2024-05-15T15:34:19.965003Z","iopub.status.idle":"2024-05-15T16:08:37.497361Z","shell.execute_reply.started":"2024-05-15T15:34:19.964970Z","shell.execute_reply":"2024-05-15T16:08:37.496163Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"k-means quantizing model into 8 bits\n    8-bit k-means quantized model has size=37.86 MiB\n    8-bit k-means quantized model has ssi loss=0.03 before quantization-aware training \n        No need for quantization-aware training since ssi loss drop=0.00 is smaller than threshold=0.02\nk-means quantizing model into 4 bits\n    4-bit k-means quantized model has size=19.39 MiB\n    4-bit k-means quantized model has ssi loss=0.08 before quantization-aware training \n        Quantization-aware training due to ssi loss drop=0.05 is larger than threshold=0.02\ndevice: cuda\nEpoch  1\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 1512/1512 [03:18<00:00,  7.63it/s, training_loss=0.0509]\nValidation: 100%|| 504/504 [00:38<00:00, 13.02it/s, validation_loss=0.0452]\n","output_type":"stream"},{"name":"stdout","text":"Model saved at : models/DPTForDepthEstimation\nEpoch  2\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 1512/1512 [03:19<00:00,  7.59it/s, training_loss=0.0425]\nValidation: 100%|| 504/504 [00:37<00:00, 13.54it/s, validation_loss=0.0411]\n","output_type":"stream"},{"name":"stdout","text":"Model saved at : models/DPTForDepthEstimation\nEpoch  3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 1512/1512 [03:18<00:00,  7.60it/s, training_loss=0.0397]\nValidation: 100%|| 504/504 [00:37<00:00, 13.46it/s, validation_loss=0.0404]\n","output_type":"stream"},{"name":"stdout","text":"Model saved at : models/DPTForDepthEstimation\nFinished Training\n    4-bit k-means quantized model has ssi loss=0.04 before quantization-aware training \nk-means quantizing model into 2 bits\n    2-bit k-means quantized model has size=9.76 MiB\n    2-bit k-means quantized model has ssi loss=0.11 before quantization-aware training \n        Quantization-aware training due to ssi loss drop=0.09 is larger than threshold=0.02\ndevice: cuda\nEpoch  1\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 1512/1512 [03:30<00:00,  7.19it/s, training_loss=0.201]\nValidation: 100%|| 504/504 [00:39<00:00, 12.86it/s, validation_loss=0.207]\n","output_type":"stream"},{"name":"stdout","text":"Model saved at : models/DPTForDepthEstimation\nEpoch  2\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 1512/1512 [03:34<00:00,  7.06it/s, training_loss=0.2]  \nValidation: 100%|| 504/504 [00:41<00:00, 12.14it/s, validation_loss=0.207]\n","output_type":"stream"},{"name":"stdout","text":"Epoch  3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 1512/1512 [03:40<00:00,  6.84it/s, training_loss=0.201]\nValidation: 100%|| 504/504 [00:42<00:00, 11.80it/s, validation_loss=0.207]\n","output_type":"stream"},{"name":"stdout","text":"Finished Training\n    2-bit k-means quantized model has ssi loss=0.20 before quantization-aware training \n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}