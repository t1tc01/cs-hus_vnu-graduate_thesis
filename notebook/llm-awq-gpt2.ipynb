{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tqdm\nimport torch\nfrom torch import nn\nfrom transformers import  GPT2Tokenizer, GPT2Model\nfrom datasets import load_dataset\nfrom functools import partial\nimport gc","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:27:54.465899Z","iopub.execute_input":"2024-05-04T13:27:54.466851Z","iopub.status.idle":"2024-05-04T13:28:01.839458Z","shell.execute_reply.started":"2024-05-04T13:27:54.466804Z","shell.execute_reply":"2024-05-04T13:28:01.838627Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Settings","metadata":{}},{"cell_type":"code","source":"testenc = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:28:01.841050Z","iopub.execute_input":"2024-05-04T13:28:01.841502Z","iopub.status.idle":"2024-05-04T13:28:08.300343Z","shell.execute_reply.started":"2024-05-04T13:28:01.841475Z","shell.execute_reply":"2024-05-04T13:28:08.299467Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"667bd5ddd0e54e2ebe46fd80d08c8d92"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 733k/733k [00:00<00:00, 4.24MB/s]\nDownloading data: 100%|██████████| 6.36M/6.36M [00:00<00:00, 28.7MB/s]\nDownloading data: 100%|██████████| 657k/657k [00:00<00:00, 5.76MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2af5b8a336e24df281ac2371046cb9a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d3151b58d0649279cbcfe5e502ba8d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79e1f2f53f1740d7a27f13fefdcb238b"}},"metadata":{}}]},{"cell_type":"code","source":"def get_model_size(model: nn.Module, data_width=16, group_size=-1):\n\n    if group_size != -1:\n        data_width += (16 + 4) / group_size\n\n    num_elements = 0\n    for param in model.parameters():\n        num_elements += param.numel()\n    return num_elements * data_width\n\nByte = 8\nKiB = 1024 * Byte\nMiB = 1024 * KiB\nGiB = 1024 * MiB","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:28:08.301458Z","iopub.execute_input":"2024-05-04T13:28:08.301736Z","iopub.status.idle":"2024-05-04T13:28:08.307566Z","shell.execute_reply.started":"2024-05-04T13:28:08.301711Z","shell.execute_reply":"2024-05-04T13:28:08.306585Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Load model","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n\ndevice = \"cuda\"\nmodel_id = \"openai-community/gpt2\"\nmodel = GPT2LMHeadModel.from_pretrained(model_id).to(device)\ntokenizer = GPT2TokenizerFast.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:28:08.310092Z","iopub.execute_input":"2024-05-04T13:28:08.310397Z","iopub.status.idle":"2024-05-04T13:28:13.170564Z","shell.execute_reply.started":"2024-05-04T13:28:08.310360Z","shell.execute_reply":"2024-05-04T13:28:13.169563Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43236faac2e34f5b9f08c0764715ac0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34908eeb72094ecba73e16f46ae54cce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ec5c8cf8dc4425e9981f10d81d2db2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b58be06eb1e6401ba4127562a382f697"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96482e0fec4f4560b6d98a5dba944e1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f21b3011b8424d32a0ddf142064e28a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44d9320ab51a4efbbc44fe7dc6990f75"}},"metadata":{}}]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:28:13.171809Z","iopub.execute_input":"2024-05-04T13:28:13.172095Z","iopub.status.idle":"2024-05-04T13:28:13.180949Z","shell.execute_reply.started":"2024-05-04T13:28:13.172069Z","shell.execute_reply":"2024-05-04T13:28:13.180023Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\n\ndef evaluate(model, tokenizer):\n    test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n    encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n    \n    max_length = model.config.n_positions\n    stride = 512\n    seq_len = encodings.input_ids.size(1)\n\n    nlls = []\n    prev_end_loc = 0\n    for begin_loc in tqdm(range(0, seq_len, stride)):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n\n        with torch.no_grad():\n            outputs = model(input_ids, labels=target_ids)\n\n            # loss is calculated using CrossEntropyLoss which averages over valid labels\n            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n            # to the left by 1.\n            neg_log_likelihood = outputs.loss\n\n        nlls.append(neg_log_likelihood)\n\n        prev_end_loc = end_loc\n        if end_loc == seq_len:\n            break\n\n    ppl = torch.exp(torch.stack(nlls).mean())\n    return ppl","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:28:13.182357Z","iopub.execute_input":"2024-05-04T13:28:13.183178Z","iopub.status.idle":"2024-05-04T13:28:13.192324Z","shell.execute_reply.started":"2024-05-04T13:28:13.183143Z","shell.execute_reply":"2024-05-04T13:28:13.191434Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Evaluate perplexity","metadata":{}},{"cell_type":"code","source":"model_perplexity = evaluate(model, tokenizer)\nprint(f\"\\nmodel perplexity: {model_perplexity:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:28:13.193496Z","iopub.execute_input":"2024-05-04T13:28:13.193830Z","iopub.status.idle":"2024-05-04T13:28:51.962403Z","shell.execute_reply.started":"2024-05-04T13:28:13.193799Z","shell.execute_reply":"2024-05-04T13:28:51.961469Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\n100%|█████████▉| 560/562 [00:33<00:00, 16.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nmodel perplexity: 25.19\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Model size","metadata":{}},{"cell_type":"code","source":"model_size = get_model_size(model, data_width=32, group_size=128)\nprint(f\"model size: {model_size/MiB:.2f} MiB\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:28:55.748377Z","iopub.execute_input":"2024-05-04T13:28:55.748763Z","iopub.status.idle":"2024-05-04T13:28:55.755289Z","shell.execute_reply.started":"2024-05-04T13:28:55.748732Z","shell.execute_reply":"2024-05-04T13:28:55.754130Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"model size: 477.02 MiB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## pseudo quant","metadata":{}},{"cell_type":"code","source":"# core quantization method (simulated quantization)\ndef pseudo_quantize_tensor(w, n_bit=4, q_group_size=-1):\n    org_w_shape = w.shape\n    if q_group_size > 0:\n        assert org_w_shape[-1] % q_group_size == 0\n        w = w.reshape(-1, q_group_size)\n\n    assert w.dim() == 2\n\n    # Calculate the maximum (\\alpha) and minimum values (\\beta) in the tensor.\n    max_val = w.amax(dim=1, keepdim=True)\n    assert max_val.dim() == 2 and max_val.size(0) == w.size(0) and max_val.size(1) == 1\n    min_val = w.amin(dim=1, keepdim=True)\n    assert min_val.dim() == 2 and min_val.size(0) == w.size(0) and min_val.size(1) == 1\n\n    # Calculate the scale factor and zero point.  (Formula 1 & 2)\n    max_int = 2 ** n_bit - 1\n    scales = (max_val - min_val).clamp(min=1e-5) / max_int\n    assert scales.shape == max_val.shape\n    zeros = (-torch.round(min_val / scales)).clamp_(0, max_int)\n    assert scales.shape == min_val.shape\n\n    assert torch.isnan(scales).sum() == 0\n    assert torch.isnan(w).sum() == 0\n\n    # Quantize W: Map values in the range [\\beta, \\alpha] to lie within [0, 2^b - 1] (Formula 3)\n    w = torch.clamp(torch.round(w / scales) + zeros, 0, max_int)\n    assert w.dim() == 2 and w.size(0) == scales.size(0) and w.size(1) == q_group_size\n\n    # Dequantize W (pseudo quantization, the inverse transformation of Formula 3)\n    w = (w - zeros) * scales\n    assert w.dim() == 2 and w.size(0) == scales.size(0) and w.size(1) == q_group_size\n\n    assert torch.isnan(w).sum() == 0\n\n    w = w.reshape(org_w_shape)\n    return w\n\n@torch.no_grad()\ndef pseudo_quantize_model_weight(\n    model, w_bit, q_group_size,\n):\n    for n, m in model.named_modules():\n        if isinstance(m, nn.Linear):\n            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:32:08.964450Z","iopub.execute_input":"2024-05-04T13:32:08.964826Z","iopub.status.idle":"2024-05-04T13:32:08.978803Z","shell.execute_reply.started":"2024-05-04T13:32:08.964797Z","shell.execute_reply":"2024-05-04T13:32:08.977776Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"del model\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:30:02.643607Z","iopub.execute_input":"2024-05-04T13:30:02.643962Z","iopub.status.idle":"2024-05-04T13:30:02.836654Z","shell.execute_reply.started":"2024-05-04T13:30:02.643934Z","shell.execute_reply":"2024-05-04T13:30:02.835608Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"pseudo_quantize_model_weight","metadata":{}},{"cell_type":"code","source":"model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\npseudo_quantize_model_weight(model, w_bit=3, q_group_size=128) #w_bit = 3, q_group_size = 128\n\n# Evaluate the model\nmodel_perplexity = evaluate(model, tokenizer)\nmodel_size = get_model_size(model, data_width=3, group_size=128)\nprint(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\nprint(f\"model size: {model_size/MiB:.2f} MiB\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:34:57.884423Z","iopub.execute_input":"2024-05-04T13:34:57.884854Z","iopub.status.idle":"2024-05-04T13:35:37.116022Z","shell.execute_reply.started":"2024-05-04T13:34:57.884821Z","shell.execute_reply":"2024-05-04T13:35:37.115130Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"100%|█████████▉| 560/562 [00:33<00:00, 16.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nmodel perplexity: 317567.75\nmodel size: 46.82 MiB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## calib model","metadata":{}},{"cell_type":"code","source":"def get_calib_dataset(tokenizer=None, n_samples=256, block_size=512):\n    dataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")\n    dataset = dataset.shuffle(seed=42)\n    samples = []\n    n_run = 0\n    for data in dataset:\n        line = data[\"text\"]\n        line = line.strip()\n        line_encoded = tokenizer.encode(line)\n        if len(line_encoded) > block_size:\n            continue\n        sample = torch.tensor([line_encoded])\n        if sample.numel() == 0:\n            continue\n        samples.append(sample)\n        n_run += 1\n        if n_run == n_samples:\n            break\n\n    # now concatenate all samples and split according to block size\n    cat_samples = torch.cat(samples, dim=1)\n    n_split = cat_samples.shape[1] // block_size\n    print(f\" * Split into {n_split} blocks\")\n    return [cat_samples[:, i*block_size:(i+1)*block_size] for i in range(n_split)]\n\n@torch.no_grad()\ndef get_calib_feat(model, tokenizer):\n    input_dict = dict()\n    def stat_input_max_hook(m, x, y, name):\n        if isinstance(x, tuple):\n            x = x[0]\n        x_max = x.view(-1, x.shape[-1]).abs().mean(dim=0).cpu().detach()\n        if name not in input_dict:\n            input_dict[name] = [x_max]\n        else:\n            input_dict[name] += [x_max]\n\n    hooks = []\n    for name, m in model.named_modules():\n        if isinstance(m, nn.Linear):\n            hooks.append(\n                m.register_forward_hook(\n                    partial(stat_input_max_hook, name=name)))\n\n    print(\"Collecting activation scales...\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    samples = get_calib_dataset(tokenizer)\n    pbar = tqdm(samples)\n    for input_ids in pbar:\n        input_ids = input_ids.to(device)\n        model(input_ids)\n\n    for hook in hooks:\n        hook.remove()\n    return input_dict","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:36:42.475468Z","iopub.execute_input":"2024-05-04T13:36:42.475852Z","iopub.status.idle":"2024-05-04T13:36:42.489748Z","shell.execute_reply.started":"2024-05-04T13:36:42.475823Z","shell.execute_reply":"2024-05-04T13:36:42.488426Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# del model\ngc.collect()\ntorch.cuda.empty_cache()\nmodel = GPT2LMHeadModel.from_pretrained(model_id).to(device)\ninput_feat = get_calib_feat(model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:36:46.692675Z","iopub.execute_input":"2024-05-04T13:36:46.693032Z","iopub.status.idle":"2024-05-04T13:37:10.483769Z","shell.execute_reply.started":"2024-05-04T13:36:46.693003Z","shell.execute_reply":"2024-05-04T13:37:10.482901Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Collecting activation scales...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/167 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02af44884ee7498aae6b101ae304402f"}},"metadata":{}},{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\nDownloading data: 100%|██████████| 471M/471M [00:01<00:00, 333MB/s]  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0a8c110573f4a0c861c20b5c7ca109d"}},"metadata":{}},{"name":"stdout","text":" * Split into 127 blocks\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 127/127 [00:03<00:00, 37.93it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"top K","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef pseudo_quantize_model_salient_weight_fp16(\n    model, w_bit, q_group_size, input_feat\n):\n    for n, m in model.named_modules():\n        if isinstance(m, nn.Linear):\n            importance = sum(input_feat[n]).float()\n\n            ############### YOUR CODE STARTS HERE ###############\n\n            # Step 1: Find 1% of the salient weight channels according to importance (hint: use torch.topk())\n            outlier_indices = torch.topk(importance, int(len(importance) * 0.01))[1]\n            assert outlier_indices.dim() == 1\n\n            ############### YOUR CODE ENDS HERE #################\n\n            # Back up the values of the salient weight channels\n            outlier = m.weight.data[:, outlier_indices].clone()\n\n            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)\n\n            ############### YOUR CODE STARTS HERE ###############\n\n            # Step 2: Restore the 1% salient weight channels to their original FP16 values\n            m.weight.data[:, outlier_indices] = outlier\n\n            ############### YOUR CODE ENDS HERE #################","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:37:56.705310Z","iopub.execute_input":"2024-05-04T13:37:56.706062Z","iopub.status.idle":"2024-05-04T13:37:56.713449Z","shell.execute_reply.started":"2024-05-04T13:37:56.706031Z","shell.execute_reply":"2024-05-04T13:37:56.712563Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# del model\ngc.collect()\ntorch.cuda.empty_cache()\nmodel = GPT2LMHeadModel.from_pretrained(model_id, device_map=\"auto\")\npseudo_quantize_model_salient_weight_fp16(model, w_bit=3, q_group_size=128, input_feat=input_feat)\n\n# Evaluate the model\nmodel_perplexity = evaluate(model, tokenizer)\nmodel_size = get_model_size(model, data_width=3, group_size=128)\nprint(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\nprint(f\"model size: {model_size/MiB:.2f} MiB\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:37:57.954835Z","iopub.execute_input":"2024-05-04T13:37:57.955649Z","iopub.status.idle":"2024-05-04T13:38:38.239554Z","shell.execute_reply.started":"2024-05-04T13:37:57.955616Z","shell.execute_reply":"2024-05-04T13:38:38.238589Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"100%|█████████▉| 560/562 [00:33<00:00, 16.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nmodel perplexity: 30.27\nmodel size: 46.82 MiB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Random select 1%","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef pseudo_quantize_model_random_weight_fp16(\n    model, w_bit, q_group_size, input_feat\n):\n    for n, m in model.named_modules():\n        if isinstance(m, nn.Linear):\n            importance = sum(input_feat[n]).float()\n\n            ############### YOUR CODE STARTS HERE ###############\n\n            # Step 1: Randomly choose 1% of the weight channels\n            outlier_mask = torch.randint(0, len(importance), (int(len(importance)*0.01), ))\n            assert outlier_mask.dim() == 1\n\n            ############### YOUR CODE ENDS HERE #################\n\n            # Back up the values of the selected weight channels\n            outlier = m.weight.data[:, outlier_mask].clone()\n\n            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)\n\n            ############### YOUR CODE STARTS HERE ###############\n\n            # Step 2: Restore the 1% selected weight channels to their original FP16 values\n            m.weight.data[:, outlier_mask] = outlier\n\n            ############### YOUR CODE ENDS HERE #################","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:39:01.294294Z","iopub.execute_input":"2024-05-04T13:39:01.294773Z","iopub.status.idle":"2024-05-04T13:39:01.306726Z","shell.execute_reply.started":"2024-05-04T13:39:01.294731Z","shell.execute_reply":"2024-05-04T13:39:01.305632Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# del model\ngc.collect()\ntorch.cuda.empty_cache()\nmodel = GPT2LMHeadModel.from_pretrained(model_id, device_map=\"auto\")\npseudo_quantize_model_random_weight_fp16(model, w_bit=3, q_group_size=128, input_feat=input_feat)\n\n# Evaluate the model\nmodel_perplexity = evaluate(model, tokenizer)\nmodel_size = get_model_size(model, data_width=3, group_size=128)\nprint(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\nprint(f\"model size: {model_size/MiB:.2f} MiB\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:39:01.673252Z","iopub.execute_input":"2024-05-04T13:39:01.673636Z","iopub.status.idle":"2024-05-04T13:39:41.429961Z","shell.execute_reply.started":"2024-05-04T13:39:01.673606Z","shell.execute_reply":"2024-05-04T13:39:41.429010Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"100%|█████████▉| 560/562 [00:33<00:00, 16.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nmodel perplexity: 321531.56\nmodel size: 46.82 MiB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Scale up","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef pseudo_quantize_model_weight_scaleup(\n    model, w_bit, q_group_size, input_feat, scale_factor\n):\n    for n, m in model.named_modules():\n        if isinstance(m, nn.Linear):\n            importance = sum(input_feat[n]).float()\n\n            ############### YOUR CODE STARTS HERE ###############\n\n            # Step 1: Find 1% of the salient weight channels\n            outlier_mask = torch.topk(importance, int(len(importance) * 0.01))[1]\n            assert outlier_mask.dim() == 1\n\n            ############### YOUR CODE ENDS HERE #################\n\n            # To simulate applying the scale factor, we can simply multiply it before quantization, and then divide by the scale factor after quantization.\n            # Scale up the values of the salient weight channels\n            m.weight.data[:, outlier_mask] *= scale_factor\n\n            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)\n\n            ############### YOUR CODE STARTS HERE ###############\n\n            # Step 2: Scale back down the values of the salient weight channels\n            m.weight.data[:, outlier_mask] /= scale_factor\n\n            ############### YOUR CODE ENDS HERE #################","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:39:51.518388Z","iopub.execute_input":"2024-05-04T13:39:51.519058Z","iopub.status.idle":"2024-05-04T13:39:51.526898Z","shell.execute_reply.started":"2024-05-04T13:39:51.519025Z","shell.execute_reply":"2024-05-04T13:39:51.525759Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# del model\ngc.collect()\ntorch.cuda.empty_cache()\nmodel = GPT2LMHeadModel.from_pretrained(model_id, device_map=\"auto\")\npseudo_quantize_model_weight_scaleup(model, w_bit=3, q_group_size=128, input_feat=input_feat, scale_factor=2)\n\n# Evaluate the model\nmodel_perplexity = evaluate(model, tokenizer)\nmodel_size = get_model_size(model, data_width=3, group_size=128)\nprint(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\nprint(f\"model size: {model_size/MiB:.2f} MiB\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:39:51.940937Z","iopub.execute_input":"2024-05-04T13:39:51.941314Z","iopub.status.idle":"2024-05-04T13:40:31.935087Z","shell.execute_reply.started":"2024-05-04T13:39:51.941284Z","shell.execute_reply":"2024-05-04T13:40:31.934208Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"100%|█████████▉| 560/562 [00:33<00:00, 16.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nmodel perplexity: 29706.69\nmodel size: 46.82 MiB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Scale factor 1 2 3 4","metadata":{}},{"cell_type":"code","source":"model = None\nfor scale_factor in [1,2,3,4]:\n    if model != None:\n        del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    model = GPT2LMHeadModel.from_pretrained(model_id, device_map=\"auto\")\n    pseudo_quantize_model_weight_scaleup(model, w_bit=3, q_group_size=128, input_feat=input_feat, scale_factor=scale_factor)\n\n    # Evaluate the model\n    model_perplexity = evaluate(model, tokenizer)\n    model_size = get_model_size(model, data_width=3, group_size=128)\n    print(f\"{scale_factor=}\")\n    print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n    print(f\"model size: {model_size/MiB:.2f} MiB\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:42:34.431076Z","iopub.execute_input":"2024-05-04T13:42:34.432044Z","iopub.status.idle":"2024-05-04T13:45:13.061604Z","shell.execute_reply.started":"2024-05-04T13:42:34.432004Z","shell.execute_reply":"2024-05-04T13:45:13.060640Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"100%|█████████▉| 560/562 [00:33<00:00, 16.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"scale_factor=1\n\nmodel perplexity: 317567.75\nmodel size: 46.82 MiB\n","output_type":"stream"},{"name":"stderr","text":"100%|█████████▉| 560/562 [00:33<00:00, 16.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"scale_factor=2\n\nmodel perplexity: 29706.69\nmodel size: 46.82 MiB\n","output_type":"stream"},{"name":"stderr","text":"100%|█████████▉| 560/562 [00:33<00:00, 16.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"scale_factor=3\n\nmodel perplexity: 1247.15\nmodel size: 46.82 MiB\n","output_type":"stream"},{"name":"stderr","text":"100%|█████████▉| 560/562 [00:33<00:00, 16.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"scale_factor=4\n\nmodel perplexity: 3306.24\nmodel size: 46.82 MiB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## fully connected ","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef scale_ln_fcs(ln, fcs, scales):\n    if not isinstance(fcs, list):\n        fcs = [fcs]\n\n    scales = scales.to(ln.weight.device)\n\n    ln.weight.div_(scales)\n    if hasattr(ln, 'bias') and ln.bias is not None:\n        ln.bias.div_(scales)\n\n    for fc in fcs:\n        fc.weight.mul_(scales.view(1, -1))\n\n    for p in ln.parameters():\n        assert torch.isnan(p).sum() == 0\n    for fc in fcs:\n        for p in fc.parameters():\n            assert torch.isnan(p).sum() == 0\n\n\n@torch.no_grad()\ndef scale_fc_fc(fc1, fc2, scales):\n    assert isinstance(fc1, nn.Linear)\n    assert isinstance(fc2, nn.Linear)\n\n    scales = scales.to(fc1.weight.device)\n\n    # fc1.weight.div_(scales.view(-1, 1))\n    fc1.weight[-scales.size(0):].div_(scales.view(-1, 1))\n    if fc1.bias is not None:\n        fc1.bias.div_(scales.view(-1))\n\n    fc2.weight.mul_(scales.view(1, -1))\n\n    for p in fc1.parameters():\n        assert torch.isnan(p).sum() == 0\n    for p in fc2.parameters():\n        assert torch.isnan(p).sum() == 0\n\n@torch.no_grad()\ndef auto_scale_block(module, name, w_bit,\n                     q_group_size,\n                     input_feat):\n\n    # find the best scale ratio\n    def _search_module_scale(block, linears2scale: list, x, kwargs={}):\n\n        x = x.to(next(block.parameters()).device)\n        with torch.no_grad():\n            org_out = block(x, **kwargs)\n            if isinstance(org_out, tuple):\n                org_out = org_out[0]\n\n        s_x = x.view(-1, x.shape[-1]).abs().mean(0)\n\n        ############### YOUR CODE STARTS HERE ###############\n\n        # Step 1: Initialize the best_error, best_ratio and best_scales\n        best_error = torch.inf\n        best_ratio = -1\n        best_scales = 0\n\n        ############### YOUR CODE ENDS HERE #################\n\n        n_grid = 20\n        history = []\n\n        org_sd = {k: v.cpu() for k, v in block.state_dict().items()}\n        for ratio in range(n_grid):\n            # ratio is the \\alpha in the formula\n            ratio = ratio * 1 / n_grid\n\n            ############### YOUR CODE STARTS HERE ###############\n\n            # Step 2: Calculate the scales by the formula: scales = s_x^ratio\n            scales = torch.clamp(s_x, 1e-5) ** ratio # must clip the s_x, otherwise will get nan later\n\n            assert scales.shape == s_x.shape\n\n            ############### YOUR CODE ENDS HERE #################\n\n            scales = scales / (scales.max() * scales.min()).sqrt().view(1, -1)\n\n            for fc in linears2scale:\n\n                scales = scales.to(fc.weight.device)\n\n                # Scale up the values of the weight channels\n                fc.weight.mul_(scales)\n\n                fc.weight.data = pseudo_quantize_tensor(fc.weight.data, w_bit, q_group_size)\n\n                ############### YOUR CODE STARTS HERE ###############\n\n                # Step 3: Scale back down the values of the weight channels\n                fc.weight.data /= scales\n\n                ############### YOUR CODE ENDS HERE #################\n\n            out = block(x, **kwargs)\n            if isinstance(out, tuple):\n                out = out[0]\n\n            loss = (org_out - out).float().pow(2).mean().item()  # float prevents overflow\n            history.append(loss)\n            is_best = loss < best_error\n            if is_best:\n                best_error = loss\n                best_ratio = ratio\n                best_scales = scales\n            block.load_state_dict(org_sd)\n\n        if best_ratio == -1:\n            print(history)\n            raise Exception\n\n        best_scales = best_scales.view(-1)\n\n        assert torch.isnan(best_scales).sum() == 0, best_scales\n        return best_scales.detach()\n\n    # attention input\n    inp = input_feat[name + '.self_attn.out_proj']\n    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0).unsqueeze(0)\n    qkv = [module.self_attn.q_proj, module.self_attn.k_proj, module.self_attn.v_proj]\n    final_scales = _search_module_scale(module.self_attn, qkv, inp)\n    scale_ln_fcs(module.self_attn_layer_norm, qkv, final_scales)\n\n    # attn out\n    inp = input_feat[name + '.self_attn.out_proj']\n    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0)\n    final_scales = _search_module_scale(module.self_attn.out_proj, [module.self_attn.out_proj], inp)\n    scale_fc_fc(module.self_attn.v_proj, module.self_attn.out_proj, final_scales)\n\n    # fc1\n    inp = input_feat[name + '.fc1']\n    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0)\n    final_scales = _search_module_scale(module.fc1, [module.fc1], inp)\n    scale_ln_fcs(module.final_layer_norm, module.fc1, final_scales)\n\n    # fc2\n    inp = input_feat[name + '.fc2']\n    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0)\n    final_scales = _search_module_scale(module.fc2, [module.fc2], inp)\n    scale_fc_fc(module.fc1, module.fc2, final_scales)\n\n@torch.no_grad()\ndef pseudo_quantize_model_weight_auto_scale(\n    model, w_bit, q_group_size, input_feat\n):\n    from transformers.models.opt.modeling_opt import OPTDecoderLayer\n\n    for name, module in model.named_modules():\n        if isinstance(module, OPTDecoderLayer):\n            auto_scale_block(module, name, w_bit, q_group_size, input_feat)\n\n    for n, m in model.named_modules():\n        if isinstance(m, nn.Linear):\n            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:45:28.660733Z","iopub.execute_input":"2024-05-04T13:45:28.661107Z","iopub.status.idle":"2024-05-04T13:45:28.691543Z","shell.execute_reply.started":"2024-05-04T13:45:28.661078Z","shell.execute_reply":"2024-05-04T13:45:28.690321Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"del model\ngc.collect()\ntorch.cuda.empty_cache()\nmodel = GPT2LMHeadModel.from_pretrained(model_id, device_map=\"auto\")\npseudo_quantize_model_weight_auto_scale(model, w_bit=3, q_group_size=128, input_feat=input_feat)\n\n# Evaluate the model\nmodel_perplexity = evaluate(model, tokenizer)\nmodel_size = get_model_size(model, data_width=3, group_size=128)\nprint(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\nprint(f\"model size: {model_size/MiB:.2f} MiB\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:45:28.978195Z","iopub.execute_input":"2024-05-04T13:45:28.978554Z","iopub.status.idle":"2024-05-04T13:46:08.628126Z","shell.execute_reply.started":"2024-05-04T13:45:28.978525Z","shell.execute_reply":"2024-05-04T13:46:08.627213Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"100%|█████████▉| 560/562 [00:33<00:00, 16.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nmodel perplexity: 317567.75\nmodel size: 46.82 MiB\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}